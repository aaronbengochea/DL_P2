{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:36:23.312729Z","iopub.execute_input":"2025-04-16T17:36:23.313027Z","iopub.status.idle":"2025-04-16T17:36:24.288871Z","shell.execute_reply.started":"2025-04-16T17:36:23.312993Z","shell.execute_reply":"2025-04-16T17:36:24.287784Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 3.56 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=2,\n    lora_alpha=6,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query', 'key'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:36:37.442654Z","iopub.execute_input":"2025-04-16T17:36:37.442950Z","iopub.status.idle":"2025-04-16T17:37:02.227040Z","shell.execute_reply.started":"2025-04-16T17:36:37.442922Z","shell.execute_reply":"2025-04-16T17:37:02.226138Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170569d1ff934350b821cd52bb46d439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf62c44dcbc540e58988ed935cff59de"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 667,396 || all params: 125,316,104 || trainable%: 0.5326\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:37:02.228033Z","iopub.execute_input":"2025-04-16T17:37:02.228639Z","iopub.status.idle":"2025-04-16T17:37:02.236565Z","shell.execute_reply.started":"2025-04-16T17:37:02.228614Z","shell.execute_reply":"2025-04-16T17:37:02.235470Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=2, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=2, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=2, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=2, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:37:10.807905Z","iopub.execute_input":"2025-04-16T17:37:10.808238Z","iopub.status.idle":"2025-04-16T17:54:50.748139Z","shell.execute_reply.started":"2025-04-16T17:37:10.808205Z","shell.execute_reply":"2025-04-16T17:54:50.747258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5dfec48bc04e1a924bfc758de17a67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36e2d1668701480781418b22e132eb6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8845546d1ae7488899458fcdd6a7ef24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bbae10b167e49b28224140191d0d00d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93d0c4d2771f4f889b5343d3cf2e7014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82affed0fde4033af0858b0f9c4792a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23c5e58c2cd4fecb3d6d3e8a1b9ff1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbe7ad5c2f645ec874acdf051871d82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649cef9a204c4281b1a2c6bf78798db7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70757110399d4015a42237845d21e075"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 16:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.374500</td>\n      <td>1.342785</td>\n      <td>0.598437</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.303100</td>\n      <td>1.227373</td>\n      <td>0.871875</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.060200</td>\n      <td>0.785594</td>\n      <td>0.864062</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.587600</td>\n      <td>0.400323</td>\n      <td>0.889062</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.391700</td>\n      <td>0.337846</td>\n      <td>0.887500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.360100</td>\n      <td>0.326827</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.334600</td>\n      <td>0.324566</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.357700</td>\n      <td>0.314877</td>\n      <td>0.895312</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.347800</td>\n      <td>0.317368</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.330400</td>\n      <td>0.315551</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.330800</td>\n      <td>0.313665</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.336500</td>\n      <td>0.313921</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.349700</td>\n      <td>0.314916</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.312700</td>\n      <td>0.315182</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.327000</td>\n      <td>0.314070</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.348200</td>\n      <td>0.312053</td>\n      <td>0.895312</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.296900</td>\n      <td>0.314224</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.308900</td>\n      <td>0.315010</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.304400</td>\n      <td>0.315106</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.357100</td>\n      <td>0.315065</td>\n      <td>0.906250</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2000'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:46.338372Z","iopub.execute_input":"2025-04-16T17:57:46.338748Z","iopub.status.idle":"2025-04-16T17:59:00.578644Z","shell.execute_reply.started":"2025-04-16T17:57:46.338718Z","shell.execute_reply":"2025-04-16T17:59:00.577773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c91edaf6d4b41c1887bc432a3b567c6"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:57<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2000.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = '/kaggle/working/saved_models/checkpoint-2000/processed_log_history.csv'\n\ndf = pd.read_csv(csv_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:15:53.807964Z","iopub.execute_input":"2025-04-15T16:15:53.808300Z","iopub.status.idle":"2025-04-15T16:15:53.834992Z","shell.execute_reply.started":"2025-04-15T16:15:53.808271Z","shell.execute_reply":"2025-04-15T16:15:53.834072Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3558   1.245366   0.397500  0.810937       0.000048  0.013405   \n1    200      0.6322   0.345979   0.835268  0.885938       0.000047  0.026810   \n2    300      0.3550   0.338247   0.876786  0.892188       0.000045  0.040214   \n3    400      0.3036   0.328299   0.898214  0.901563       0.000043  0.053619   \n4    500      0.3174   0.316743   0.895536  0.906250       0.000042  0.067024   \n5    600      0.2914   0.323829   0.903125  0.896875       0.000040  0.080429   \n6    700      0.3101   0.308364   0.899107  0.912500       0.000038  0.093834   \n7    800      0.3165   0.300734   0.895536  0.907813       0.000037  0.107239   \n8    900      0.2881   0.307563   0.905357  0.903125       0.000035  0.120643   \n9   1000      0.2862   0.302891   0.906250  0.901563       0.000033  0.134048   \n10  1100      0.3049   0.292337   0.903571  0.912500       0.000032  0.147453   \n11  1200      0.3012   0.291869   0.900000  0.909375       0.000030  0.160858   \n12  1300      0.3201   0.299127   0.900893  0.903125       0.000028  0.174263   \n13  1400      0.2931   0.291853   0.907143  0.907813       0.000027  0.187668   \n14  1500      0.3017   0.298660   0.901786  0.901563       0.000025  0.201072   \n15  1600      0.3099   0.292654   0.901339  0.906250       0.000023  0.214477   \n16  1700      0.2593   0.297433   0.911161  0.912500       0.000022  0.227882   \n17  1800      0.2826   0.293084   0.911161  0.910937       0.000020  0.241287   \n18  1900      0.2692   0.291764   0.910268  0.907813       0.000018  0.254692   \n19  2000      0.3197   0.291878   0.904911  0.912500       0.000017  0.268097   \n20  2100      0.2947   0.287986   0.900446  0.910937       0.000015  0.281501   \n21  2200      0.2839   0.286427   0.906696  0.912500       0.000013  0.294906   \n22  2300      0.2539   0.298362   0.911161  0.896875       0.000012  0.308311   \n23  2400      0.2892   0.292161   0.902232  0.901563       0.000010  0.321716   \n24  2500      0.2671   0.287797   0.900893  0.915625       0.000008  0.335121   \n25  2600      0.2542   0.287052   0.916518  0.912500       0.000007  0.348525   \n26  2700      0.2574   0.289237   0.913393  0.914062       0.000005  0.361930   \n27  2800      0.2560   0.285028   0.913393  0.915625       0.000003  0.375335   \n28  2900      0.2623   0.287193   0.910714  0.914062       0.000002  0.388740   \n29  3000      0.2497   0.286760   0.914732  0.914062       0.000000  0.402145   \n30  3100      0.2794   0.283299   0.911250  0.910937       0.000011  0.415550   \n31  3200      0.2665   0.285942   0.909375  0.915625       0.000010  0.428954   \n32  3300      0.2375   0.289334   0.918304  0.912500       0.000009  0.442359   \n33  3400      0.2276   0.291630   0.924554  0.917188       0.000008  0.455764   \n34  3500      0.2692   0.289263   0.908929  0.914062       0.000006  0.469169   \n35  3600      0.2801   0.288613   0.912946  0.915625       0.000005  0.482574   \n36  3700      0.2512   0.287335   0.918304  0.917188       0.000004  0.495979   \n37  3800      0.2385   0.286974   0.921875  0.917188       0.000003  0.509383   \n38  3900      0.2722   0.287824   0.909821  0.918750       0.000001  0.522788   \n39  4000      0.2870   0.287646   0.914732  0.921875       0.000000  0.536193   \n\n    Loss Spread  Loss Ratio  Acc Spread  Acc Ratio  \n0      0.110434    1.088676   -0.413437   0.490173  \n1      0.286221    1.827279   -0.050670   0.942807  \n2      0.016753    1.049528   -0.015402   0.982737  \n3     -0.024699    0.924768   -0.003348   0.996286  \n4      0.000657    1.002074   -0.010714   0.988177  \n5     -0.032429    0.899859    0.006250   1.006969  \n6      0.001736    1.005630   -0.013393   0.985323  \n7      0.015766    1.052426   -0.012277   0.986477  \n8     -0.019463    0.936720    0.002232   1.002472  \n9     -0.016691    0.944895    0.004687   1.005199  \n10     0.012563    1.042975   -0.008929   0.990215  \n11     0.009331    1.031970   -0.009375   0.989691  \n12     0.020973    1.070113   -0.002232   0.997528  \n13     0.001247    1.004271   -0.000670   0.999262  \n14     0.003040    1.010179    0.000223   1.000248  \n15     0.017246    1.058931   -0.004911   0.994581  \n16    -0.038133    0.871793   -0.001339   0.998532  \n17    -0.010484    0.964229    0.000223   1.000245  \n18    -0.022564    0.922663    0.002455   1.002705  \n19     0.027822    1.095322   -0.007589   0.991683  \n20     0.006714    1.023313   -0.010491   0.988483  \n21    -0.002527    0.991178   -0.005804   0.993640  \n22    -0.044462    0.850980    0.014286   1.015928  \n23    -0.002961    0.989864    0.000670   1.000743  \n24    -0.020697    0.928086   -0.014732   0.983910  \n25    -0.032852    0.885554    0.004018   1.004403  \n26    -0.031837    0.889926   -0.000670   0.999267  \n27    -0.029028    0.898156   -0.002232   0.997562  \n28    -0.024893    0.913323   -0.003348   0.996337  \n29    -0.037060    0.870763    0.000670   1.000733  \n30    -0.003899    0.986239    0.000313   1.000343  \n31    -0.019442    0.932009   -0.006250   0.993174  \n32    -0.051834    0.820850    0.005804   1.006360  \n33    -0.064030    0.780442    0.007366   1.008031  \n34    -0.020063    0.930642   -0.005134   0.994383  \n35    -0.008513    0.970503   -0.002679   0.997075  \n36    -0.036135    0.874242    0.001116   1.001217  \n37    -0.048474    0.831084    0.004687   1.005111  \n38    -0.015624    0.945716   -0.008929   0.990282  \n39    -0.000646    0.997755   -0.007143   0.992252  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>Loss Spread</th>\n      <th>Loss Ratio</th>\n      <th>Acc Spread</th>\n      <th>Acc Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3558</td>\n      <td>1.245366</td>\n      <td>0.397500</td>\n      <td>0.810937</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.110434</td>\n      <td>1.088676</td>\n      <td>-0.413437</td>\n      <td>0.490173</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.6322</td>\n      <td>0.345979</td>\n      <td>0.835268</td>\n      <td>0.885938</td>\n      <td>0.000047</td>\n      <td>0.026810</td>\n      <td>0.286221</td>\n      <td>1.827279</td>\n      <td>-0.050670</td>\n      <td>0.942807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.3550</td>\n      <td>0.338247</td>\n      <td>0.876786</td>\n      <td>0.892188</td>\n      <td>0.000045</td>\n      <td>0.040214</td>\n      <td>0.016753</td>\n      <td>1.049528</td>\n      <td>-0.015402</td>\n      <td>0.982737</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.3036</td>\n      <td>0.328299</td>\n      <td>0.898214</td>\n      <td>0.901563</td>\n      <td>0.000043</td>\n      <td>0.053619</td>\n      <td>-0.024699</td>\n      <td>0.924768</td>\n      <td>-0.003348</td>\n      <td>0.996286</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.3174</td>\n      <td>0.316743</td>\n      <td>0.895536</td>\n      <td>0.906250</td>\n      <td>0.000042</td>\n      <td>0.067024</td>\n      <td>0.000657</td>\n      <td>1.002074</td>\n      <td>-0.010714</td>\n      <td>0.988177</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.2914</td>\n      <td>0.323829</td>\n      <td>0.903125</td>\n      <td>0.896875</td>\n      <td>0.000040</td>\n      <td>0.080429</td>\n      <td>-0.032429</td>\n      <td>0.899859</td>\n      <td>0.006250</td>\n      <td>1.006969</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3101</td>\n      <td>0.308364</td>\n      <td>0.899107</td>\n      <td>0.912500</td>\n      <td>0.000038</td>\n      <td>0.093834</td>\n      <td>0.001736</td>\n      <td>1.005630</td>\n      <td>-0.013393</td>\n      <td>0.985323</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3165</td>\n      <td>0.300734</td>\n      <td>0.895536</td>\n      <td>0.907813</td>\n      <td>0.000037</td>\n      <td>0.107239</td>\n      <td>0.015766</td>\n      <td>1.052426</td>\n      <td>-0.012277</td>\n      <td>0.986477</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.2881</td>\n      <td>0.307563</td>\n      <td>0.905357</td>\n      <td>0.903125</td>\n      <td>0.000035</td>\n      <td>0.120643</td>\n      <td>-0.019463</td>\n      <td>0.936720</td>\n      <td>0.002232</td>\n      <td>1.002472</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2862</td>\n      <td>0.302891</td>\n      <td>0.906250</td>\n      <td>0.901563</td>\n      <td>0.000033</td>\n      <td>0.134048</td>\n      <td>-0.016691</td>\n      <td>0.944895</td>\n      <td>0.004687</td>\n      <td>1.005199</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.3049</td>\n      <td>0.292337</td>\n      <td>0.903571</td>\n      <td>0.912500</td>\n      <td>0.000032</td>\n      <td>0.147453</td>\n      <td>0.012563</td>\n      <td>1.042975</td>\n      <td>-0.008929</td>\n      <td>0.990215</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3012</td>\n      <td>0.291869</td>\n      <td>0.900000</td>\n      <td>0.909375</td>\n      <td>0.000030</td>\n      <td>0.160858</td>\n      <td>0.009331</td>\n      <td>1.031970</td>\n      <td>-0.009375</td>\n      <td>0.989691</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3201</td>\n      <td>0.299127</td>\n      <td>0.900893</td>\n      <td>0.903125</td>\n      <td>0.000028</td>\n      <td>0.174263</td>\n      <td>0.020973</td>\n      <td>1.070113</td>\n      <td>-0.002232</td>\n      <td>0.997528</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2931</td>\n      <td>0.291853</td>\n      <td>0.907143</td>\n      <td>0.907813</td>\n      <td>0.000027</td>\n      <td>0.187668</td>\n      <td>0.001247</td>\n      <td>1.004271</td>\n      <td>-0.000670</td>\n      <td>0.999262</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.3017</td>\n      <td>0.298660</td>\n      <td>0.901786</td>\n      <td>0.901563</td>\n      <td>0.000025</td>\n      <td>0.201072</td>\n      <td>0.003040</td>\n      <td>1.010179</td>\n      <td>0.000223</td>\n      <td>1.000248</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3099</td>\n      <td>0.292654</td>\n      <td>0.901339</td>\n      <td>0.906250</td>\n      <td>0.000023</td>\n      <td>0.214477</td>\n      <td>0.017246</td>\n      <td>1.058931</td>\n      <td>-0.004911</td>\n      <td>0.994581</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2593</td>\n      <td>0.297433</td>\n      <td>0.911161</td>\n      <td>0.912500</td>\n      <td>0.000022</td>\n      <td>0.227882</td>\n      <td>-0.038133</td>\n      <td>0.871793</td>\n      <td>-0.001339</td>\n      <td>0.998532</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2826</td>\n      <td>0.293084</td>\n      <td>0.911161</td>\n      <td>0.910937</td>\n      <td>0.000020</td>\n      <td>0.241287</td>\n      <td>-0.010484</td>\n      <td>0.964229</td>\n      <td>0.000223</td>\n      <td>1.000245</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2692</td>\n      <td>0.291764</td>\n      <td>0.910268</td>\n      <td>0.907813</td>\n      <td>0.000018</td>\n      <td>0.254692</td>\n      <td>-0.022564</td>\n      <td>0.922663</td>\n      <td>0.002455</td>\n      <td>1.002705</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3197</td>\n      <td>0.291878</td>\n      <td>0.904911</td>\n      <td>0.912500</td>\n      <td>0.000017</td>\n      <td>0.268097</td>\n      <td>0.027822</td>\n      <td>1.095322</td>\n      <td>-0.007589</td>\n      <td>0.991683</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2100</td>\n      <td>0.2947</td>\n      <td>0.287986</td>\n      <td>0.900446</td>\n      <td>0.910937</td>\n      <td>0.000015</td>\n      <td>0.281501</td>\n      <td>0.006714</td>\n      <td>1.023313</td>\n      <td>-0.010491</td>\n      <td>0.988483</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2200</td>\n      <td>0.2839</td>\n      <td>0.286427</td>\n      <td>0.906696</td>\n      <td>0.912500</td>\n      <td>0.000013</td>\n      <td>0.294906</td>\n      <td>-0.002527</td>\n      <td>0.991178</td>\n      <td>-0.005804</td>\n      <td>0.993640</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2300</td>\n      <td>0.2539</td>\n      <td>0.298362</td>\n      <td>0.911161</td>\n      <td>0.896875</td>\n      <td>0.000012</td>\n      <td>0.308311</td>\n      <td>-0.044462</td>\n      <td>0.850980</td>\n      <td>0.014286</td>\n      <td>1.015928</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2400</td>\n      <td>0.2892</td>\n      <td>0.292161</td>\n      <td>0.902232</td>\n      <td>0.901563</td>\n      <td>0.000010</td>\n      <td>0.321716</td>\n      <td>-0.002961</td>\n      <td>0.989864</td>\n      <td>0.000670</td>\n      <td>1.000743</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2500</td>\n      <td>0.2671</td>\n      <td>0.287797</td>\n      <td>0.900893</td>\n      <td>0.915625</td>\n      <td>0.000008</td>\n      <td>0.335121</td>\n      <td>-0.020697</td>\n      <td>0.928086</td>\n      <td>-0.014732</td>\n      <td>0.983910</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2600</td>\n      <td>0.2542</td>\n      <td>0.287052</td>\n      <td>0.916518</td>\n      <td>0.912500</td>\n      <td>0.000007</td>\n      <td>0.348525</td>\n      <td>-0.032852</td>\n      <td>0.885554</td>\n      <td>0.004018</td>\n      <td>1.004403</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2700</td>\n      <td>0.2574</td>\n      <td>0.289237</td>\n      <td>0.913393</td>\n      <td>0.914062</td>\n      <td>0.000005</td>\n      <td>0.361930</td>\n      <td>-0.031837</td>\n      <td>0.889926</td>\n      <td>-0.000670</td>\n      <td>0.999267</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2800</td>\n      <td>0.2560</td>\n      <td>0.285028</td>\n      <td>0.913393</td>\n      <td>0.915625</td>\n      <td>0.000003</td>\n      <td>0.375335</td>\n      <td>-0.029028</td>\n      <td>0.898156</td>\n      <td>-0.002232</td>\n      <td>0.997562</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2900</td>\n      <td>0.2623</td>\n      <td>0.287193</td>\n      <td>0.910714</td>\n      <td>0.914062</td>\n      <td>0.000002</td>\n      <td>0.388740</td>\n      <td>-0.024893</td>\n      <td>0.913323</td>\n      <td>-0.003348</td>\n      <td>0.996337</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3000</td>\n      <td>0.2497</td>\n      <td>0.286760</td>\n      <td>0.914732</td>\n      <td>0.914062</td>\n      <td>0.000000</td>\n      <td>0.402145</td>\n      <td>-0.037060</td>\n      <td>0.870763</td>\n      <td>0.000670</td>\n      <td>1.000733</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>3100</td>\n      <td>0.2794</td>\n      <td>0.283299</td>\n      <td>0.911250</td>\n      <td>0.910937</td>\n      <td>0.000011</td>\n      <td>0.415550</td>\n      <td>-0.003899</td>\n      <td>0.986239</td>\n      <td>0.000313</td>\n      <td>1.000343</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>3200</td>\n      <td>0.2665</td>\n      <td>0.285942</td>\n      <td>0.909375</td>\n      <td>0.915625</td>\n      <td>0.000010</td>\n      <td>0.428954</td>\n      <td>-0.019442</td>\n      <td>0.932009</td>\n      <td>-0.006250</td>\n      <td>0.993174</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>3300</td>\n      <td>0.2375</td>\n      <td>0.289334</td>\n      <td>0.918304</td>\n      <td>0.912500</td>\n      <td>0.000009</td>\n      <td>0.442359</td>\n      <td>-0.051834</td>\n      <td>0.820850</td>\n      <td>0.005804</td>\n      <td>1.006360</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>3400</td>\n      <td>0.2276</td>\n      <td>0.291630</td>\n      <td>0.924554</td>\n      <td>0.917188</td>\n      <td>0.000008</td>\n      <td>0.455764</td>\n      <td>-0.064030</td>\n      <td>0.780442</td>\n      <td>0.007366</td>\n      <td>1.008031</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>3500</td>\n      <td>0.2692</td>\n      <td>0.289263</td>\n      <td>0.908929</td>\n      <td>0.914062</td>\n      <td>0.000006</td>\n      <td>0.469169</td>\n      <td>-0.020063</td>\n      <td>0.930642</td>\n      <td>-0.005134</td>\n      <td>0.994383</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>3600</td>\n      <td>0.2801</td>\n      <td>0.288613</td>\n      <td>0.912946</td>\n      <td>0.915625</td>\n      <td>0.000005</td>\n      <td>0.482574</td>\n      <td>-0.008513</td>\n      <td>0.970503</td>\n      <td>-0.002679</td>\n      <td>0.997075</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>3700</td>\n      <td>0.2512</td>\n      <td>0.287335</td>\n      <td>0.918304</td>\n      <td>0.917188</td>\n      <td>0.000004</td>\n      <td>0.495979</td>\n      <td>-0.036135</td>\n      <td>0.874242</td>\n      <td>0.001116</td>\n      <td>1.001217</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>3800</td>\n      <td>0.2385</td>\n      <td>0.286974</td>\n      <td>0.921875</td>\n      <td>0.917188</td>\n      <td>0.000003</td>\n      <td>0.509383</td>\n      <td>-0.048474</td>\n      <td>0.831084</td>\n      <td>0.004687</td>\n      <td>1.005111</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>3900</td>\n      <td>0.2722</td>\n      <td>0.287824</td>\n      <td>0.909821</td>\n      <td>0.918750</td>\n      <td>0.000001</td>\n      <td>0.522788</td>\n      <td>-0.015624</td>\n      <td>0.945716</td>\n      <td>-0.008929</td>\n      <td>0.990282</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>4000</td>\n      <td>0.2870</td>\n      <td>0.287646</td>\n      <td>0.914732</td>\n      <td>0.921875</td>\n      <td>0.000000</td>\n      <td>0.536193</td>\n      <td>-0.000646</td>\n      <td>0.997755</td>\n      <td>-0.007143</td>\n      <td>0.992252</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}