{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:58:06.150227Z","iopub.execute_input":"2025-04-11T15:58:06.150524Z","iopub.status.idle":"2025-04-11T15:58:07.075304Z","shell.execute_reply.started":"2025-04-11T15:58:06.150486Z","shell.execute_reply":"2025-04-11T15:58:07.074268Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 27, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (19/19), done.\u001b[K\nremote: Total 27 (delta 10), reused 23 (delta 6), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (27/27), 6.89 KiB | 2.30 MiB/s, done.\nResolving deltas: 100% (10/10), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=6,\n    lora_alpha=18,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query','key','value'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:58:43.190419Z","iopub.execute_input":"2025-04-11T15:58:43.190794Z","iopub.status.idle":"2025-04-11T15:59:08.537009Z","shell.execute_reply.started":"2025-04-11T15:58:43.190763Z","shell.execute_reply":"2025-04-11T15:59:08.536104Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3653d2bfac58441ea3b6320841c9eb3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd79252b7234ca885bfb2a11f549c45"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:59:08.538035Z","iopub.execute_input":"2025-04-11T15:59:08.538477Z","iopub.status.idle":"2025-04-11T15:59:08.546558Z","shell.execute_reply.started":"2025-04-11T15:59:08.538456Z","shell.execute_reply":"2025-04-11T15:59:08.545625Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='rmsprop', \n    learning_rate=5e-5,\n    lr_scheduler_type='cosine',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:59:14.851864Z","iopub.execute_input":"2025-04-11T15:59:14.852171Z","iopub.status.idle":"2025-04-11T16:17:12.986742Z","shell.execute_reply.started":"2025-04-11T15:59:14.852150Z","shell.execute_reply":"2025-04-11T16:17:12.985829Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eba45091b9d42e8bd0c421f06a6b1f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ad8d820636470987a28b5ded8e3b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e158d99d2ad4b5d8955b4c5914a5109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba75b9844c549ebafedb286822f04d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089db43a4eb142b1a8cef66cedc4131a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ccd213d8a6f4caf8316f8e6405b5e12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a26acb31d84584a0db91274f17933e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6734e6deff4518a4e619c9370dfbd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a58e47744eca46ad8c7e3679e95f96e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a8e83a192149ac9efe2c4df2ba5841"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 16:43, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.725000</td>\n      <td>0.334776</td>\n      <td>0.890625</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.355700</td>\n      <td>0.331924</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.330500</td>\n      <td>0.359688</td>\n      <td>0.887500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.300100</td>\n      <td>0.333891</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.294300</td>\n      <td>0.333229</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.280000</td>\n      <td>0.320839</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.301400</td>\n      <td>0.333608</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.293600</td>\n      <td>0.316424</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.284600</td>\n      <td>0.320424</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.285900</td>\n      <td>0.316118</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.296700</td>\n      <td>0.311318</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.300400</td>\n      <td>0.311173</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.316800</td>\n      <td>0.311043</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.285000</td>\n      <td>0.309274</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.290300</td>\n      <td>0.306422</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.316300</td>\n      <td>0.304859</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.259200</td>\n      <td>0.309112</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.276300</td>\n      <td>0.308641</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.257600</td>\n      <td>0.308038</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.321300</td>\n      <td>0.307978</td>\n      <td>0.909375</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2000'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T16:17:12.989034Z","iopub.execute_input":"2025-04-11T16:17:12.989291Z","iopub.status.idle":"2025-04-11T16:18:19.980331Z","shell.execute_reply.started":"2025-04-11T16:17:12.989268Z","shell.execute_reply":"2025-04-11T16:18:19.979406Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25035904bde482c868ef98fde2ba416"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.22it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2000.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define paths\ncsv_path = \"/kaggle/working/saved_models/checkpoint-2000/log_history.csv\"\noutput_dir = \"/kaggle/working/processed_data\"\noutput_csv = os.path.join(output_dir, \"processed_log_history.csv\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read the csv file\ndf = pd.read_csv(csv_path)\n\n# Select desired columns and reorder the dataframe\ndesired_order = [\n    \"step\",         \n    \"loss\",         \n    \"eval_loss\",   \n    \"accuracy\",     \n    \"eval_accuracy\",\n    \"learning_rate\",\n    \"epoch\"         \n]\ndf = df[desired_order]\n\n# Rename columns for uniformity\ndf.rename(columns={\n    \"step\": \"Step\",\n    \"loss\": \"Train Loss\",\n    \"eval_loss\": \"Test Loss\",\n    \"accuracy\": \"Train Acc\",\n    \"eval_accuracy\": \"Test Acc\",\n    \"learning_rate\": \"Learning Rate\",\n    \"epoch\": \"Epochs\"\n}, inplace=True)\n\n# Adding loss spread and loss ratio columns\ndf[\"loss spread\"] = df[\"Train Loss\"] - df[\"Test Loss\"]\ndf[\"loss ratio\"] = df[\"Train Loss\"] / df[\"Test Loss\"]\n\n# Adding acc spread and acc ratio columns\ndf[\"Acc spread\"] = df[\"Test Acc\"] - df[\"Train Acc\"]\ndf[\"Acc ratio\"] = df[\"Test Acc\"] / df[\"Train Acc\"]\n\n# Export the DataFrame as a csv file\ndf.to_csv(output_csv, index=False)\n\nprint(f\"Data exported to {output_csv}\")\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T16:19:13.979075Z","iopub.execute_input":"2025-04-11T16:19:13.979382Z","iopub.status.idle":"2025-04-11T16:19:14.008108Z","shell.execute_reply.started":"2025-04-11T16:19:13.979360Z","shell.execute_reply":"2025-04-11T16:19:14.007257Z"}},"outputs":[{"name":"stdout","text":"Data exported to /kaggle/working/processed_data/processed_log_history.csv\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      0.7250   0.334776   0.736875  0.890625   4.969221e-05  0.013405   \n1    200      0.3557   0.331924   0.889732  0.900000   4.877641e-05  0.026810   \n2    300      0.3305   0.359688   0.889286  0.887500   4.727516e-05  0.040214   \n3    400      0.3001   0.333891   0.893750  0.901563   4.522542e-05  0.053619   \n4    500      0.2943   0.333229   0.899554  0.903125   4.267767e-05  0.067024   \n5    600      0.2800   0.320839   0.910714  0.909375   3.969463e-05  0.080429   \n6    700      0.3014   0.333608   0.908482  0.900000   3.634976e-05  0.093834   \n7    800      0.2936   0.316424   0.899554  0.910937   3.272542e-05  0.107239   \n8    900      0.2846   0.320424   0.904911  0.907813   2.891086e-05  0.120643   \n9   1000      0.2859   0.316118   0.903571  0.904687   2.500000e-05  0.134048   \n10  1100      0.2967   0.311318   0.901339  0.901563   2.108914e-05  0.147453   \n11  1200      0.3004   0.311173   0.898661  0.909375   1.727458e-05  0.160858   \n12  1300      0.3168   0.311043   0.906250  0.907813   1.365024e-05  0.174263   \n13  1400      0.2850   0.309274   0.907589  0.901563   1.030537e-05  0.187668   \n14  1500      0.2903   0.306422   0.904018  0.910937   7.322330e-06  0.201072   \n15  1600      0.3163   0.304859   0.903125  0.912500   4.774575e-06  0.214477   \n16  1700      0.2592   0.309112   0.911607  0.910937   2.724837e-06  0.227882   \n17  1800      0.2763   0.308641   0.907589  0.910937   1.223587e-06  0.241287   \n18  1900      0.2576   0.308038   0.912500  0.910937   3.077915e-07  0.254692   \n19  2000      0.3213   0.307978   0.905804  0.909375   0.000000e+00  0.268097   \n\n    loss spread  loss ratio  Acc spread  Acc ratio  \n0      0.390224    2.165627    0.153750   1.208651  \n1      0.023776    1.071631    0.010268   1.011540  \n2     -0.029188    0.918853   -0.001786   0.997992  \n3     -0.033791    0.898795    0.007812   1.008741  \n4     -0.038929    0.883177    0.003571   1.003970  \n5     -0.040839    0.872712   -0.001339   0.998529  \n6     -0.032208    0.903457   -0.008482   0.990663  \n7     -0.022824    0.927870    0.011384   1.012655  \n8     -0.035824    0.888198    0.002902   1.003207  \n9     -0.030218    0.904409    0.001116   1.001235  \n10    -0.014618    0.953045    0.000223   1.000248  \n11    -0.010773    0.965379    0.010714   1.011923  \n12     0.005757    1.018508    0.001563   1.001724  \n13    -0.024274    0.921513   -0.006027   0.993360  \n14    -0.016122    0.947385    0.006920   1.007654  \n15     0.011441    1.037527    0.009375   1.010381  \n16    -0.049912    0.838531   -0.000670   0.999265  \n17    -0.032341    0.895214    0.003348   1.003689  \n18    -0.050438    0.836261   -0.001563   0.998288  \n19     0.013322    1.043258    0.003571   1.003943  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>loss spread</th>\n      <th>loss ratio</th>\n      <th>Acc spread</th>\n      <th>Acc ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.7250</td>\n      <td>0.334776</td>\n      <td>0.736875</td>\n      <td>0.890625</td>\n      <td>4.969221e-05</td>\n      <td>0.013405</td>\n      <td>0.390224</td>\n      <td>2.165627</td>\n      <td>0.153750</td>\n      <td>1.208651</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.3557</td>\n      <td>0.331924</td>\n      <td>0.889732</td>\n      <td>0.900000</td>\n      <td>4.877641e-05</td>\n      <td>0.026810</td>\n      <td>0.023776</td>\n      <td>1.071631</td>\n      <td>0.010268</td>\n      <td>1.011540</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.3305</td>\n      <td>0.359688</td>\n      <td>0.889286</td>\n      <td>0.887500</td>\n      <td>4.727516e-05</td>\n      <td>0.040214</td>\n      <td>-0.029188</td>\n      <td>0.918853</td>\n      <td>-0.001786</td>\n      <td>0.997992</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.3001</td>\n      <td>0.333891</td>\n      <td>0.893750</td>\n      <td>0.901563</td>\n      <td>4.522542e-05</td>\n      <td>0.053619</td>\n      <td>-0.033791</td>\n      <td>0.898795</td>\n      <td>0.007812</td>\n      <td>1.008741</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.2943</td>\n      <td>0.333229</td>\n      <td>0.899554</td>\n      <td>0.903125</td>\n      <td>4.267767e-05</td>\n      <td>0.067024</td>\n      <td>-0.038929</td>\n      <td>0.883177</td>\n      <td>0.003571</td>\n      <td>1.003970</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.2800</td>\n      <td>0.320839</td>\n      <td>0.910714</td>\n      <td>0.909375</td>\n      <td>3.969463e-05</td>\n      <td>0.080429</td>\n      <td>-0.040839</td>\n      <td>0.872712</td>\n      <td>-0.001339</td>\n      <td>0.998529</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3014</td>\n      <td>0.333608</td>\n      <td>0.908482</td>\n      <td>0.900000</td>\n      <td>3.634976e-05</td>\n      <td>0.093834</td>\n      <td>-0.032208</td>\n      <td>0.903457</td>\n      <td>-0.008482</td>\n      <td>0.990663</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.2936</td>\n      <td>0.316424</td>\n      <td>0.899554</td>\n      <td>0.910937</td>\n      <td>3.272542e-05</td>\n      <td>0.107239</td>\n      <td>-0.022824</td>\n      <td>0.927870</td>\n      <td>0.011384</td>\n      <td>1.012655</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.2846</td>\n      <td>0.320424</td>\n      <td>0.904911</td>\n      <td>0.907813</td>\n      <td>2.891086e-05</td>\n      <td>0.120643</td>\n      <td>-0.035824</td>\n      <td>0.888198</td>\n      <td>0.002902</td>\n      <td>1.003207</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2859</td>\n      <td>0.316118</td>\n      <td>0.903571</td>\n      <td>0.904687</td>\n      <td>2.500000e-05</td>\n      <td>0.134048</td>\n      <td>-0.030218</td>\n      <td>0.904409</td>\n      <td>0.001116</td>\n      <td>1.001235</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.2967</td>\n      <td>0.311318</td>\n      <td>0.901339</td>\n      <td>0.901563</td>\n      <td>2.108914e-05</td>\n      <td>0.147453</td>\n      <td>-0.014618</td>\n      <td>0.953045</td>\n      <td>0.000223</td>\n      <td>1.000248</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3004</td>\n      <td>0.311173</td>\n      <td>0.898661</td>\n      <td>0.909375</td>\n      <td>1.727458e-05</td>\n      <td>0.160858</td>\n      <td>-0.010773</td>\n      <td>0.965379</td>\n      <td>0.010714</td>\n      <td>1.011923</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3168</td>\n      <td>0.311043</td>\n      <td>0.906250</td>\n      <td>0.907813</td>\n      <td>1.365024e-05</td>\n      <td>0.174263</td>\n      <td>0.005757</td>\n      <td>1.018508</td>\n      <td>0.001563</td>\n      <td>1.001724</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2850</td>\n      <td>0.309274</td>\n      <td>0.907589</td>\n      <td>0.901563</td>\n      <td>1.030537e-05</td>\n      <td>0.187668</td>\n      <td>-0.024274</td>\n      <td>0.921513</td>\n      <td>-0.006027</td>\n      <td>0.993360</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.2903</td>\n      <td>0.306422</td>\n      <td>0.904018</td>\n      <td>0.910937</td>\n      <td>7.322330e-06</td>\n      <td>0.201072</td>\n      <td>-0.016122</td>\n      <td>0.947385</td>\n      <td>0.006920</td>\n      <td>1.007654</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3163</td>\n      <td>0.304859</td>\n      <td>0.903125</td>\n      <td>0.912500</td>\n      <td>4.774575e-06</td>\n      <td>0.214477</td>\n      <td>0.011441</td>\n      <td>1.037527</td>\n      <td>0.009375</td>\n      <td>1.010381</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2592</td>\n      <td>0.309112</td>\n      <td>0.911607</td>\n      <td>0.910937</td>\n      <td>2.724837e-06</td>\n      <td>0.227882</td>\n      <td>-0.049912</td>\n      <td>0.838531</td>\n      <td>-0.000670</td>\n      <td>0.999265</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2763</td>\n      <td>0.308641</td>\n      <td>0.907589</td>\n      <td>0.910937</td>\n      <td>1.223587e-06</td>\n      <td>0.241287</td>\n      <td>-0.032341</td>\n      <td>0.895214</td>\n      <td>0.003348</td>\n      <td>1.003689</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2576</td>\n      <td>0.308038</td>\n      <td>0.912500</td>\n      <td>0.910937</td>\n      <td>3.077915e-07</td>\n      <td>0.254692</td>\n      <td>-0.050438</td>\n      <td>0.836261</td>\n      <td>-0.001563</td>\n      <td>0.998288</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3213</td>\n      <td>0.307978</td>\n      <td>0.905804</td>\n      <td>0.909375</td>\n      <td>0.000000e+00</td>\n      <td>0.268097</td>\n      <td>0.013322</td>\n      <td>1.043258</td>\n      <td>0.003571</td>\n      <td>1.003943</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7}]}