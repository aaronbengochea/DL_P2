{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:58:06.150227Z","iopub.execute_input":"2025-04-11T15:58:06.150524Z","iopub.status.idle":"2025-04-11T15:58:07.075304Z","shell.execute_reply.started":"2025-04-11T15:58:06.150486Z","shell.execute_reply":"2025-04-11T15:58:07.074268Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 27, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (19/19), done.\u001b[K\nremote: Total 27 (delta 10), reused 23 (delta 6), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (27/27), 6.89 KiB | 2.30 MiB/s, done.\nResolving deltas: 100% (10/10), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=6,\n    lora_alpha=18,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query','key','value'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:21:37.485180Z","iopub.execute_input":"2025-04-11T17:21:37.485514Z","iopub.status.idle":"2025-04-11T17:21:37.800216Z","shell.execute_reply.started":"2025-04-11T17:21:37.485488Z","shell.execute_reply":"2025-04-11T17:21:37.799371Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:21:37.989268Z","iopub.execute_input":"2025-04-11T17:21:37.989622Z","iopub.status.idle":"2025-04-11T17:21:37.998231Z","shell.execute_reply.started":"2025-04-11T17:21:37.989591Z","shell.execute_reply":"2025-04-11T17:21:37.997339Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='sgd',\n    learning_rate=5e-3,\n    lr_scheduler_type='cosine',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:21:40.251752Z","iopub.execute_input":"2025-04-11T17:21:40.252056Z","iopub.status.idle":"2025-04-11T17:38:31.026095Z","shell.execute_reply.started":"2025-04-11T17:21:40.252033Z","shell.execute_reply":"2025-04-11T17:38:31.025127Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 16:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.388900</td>\n      <td>1.386846</td>\n      <td>0.226562</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.385400</td>\n      <td>1.380215</td>\n      <td>0.287500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.383500</td>\n      <td>1.378506</td>\n      <td>0.418750</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.381500</td>\n      <td>1.384044</td>\n      <td>0.259375</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.382700</td>\n      <td>1.375127</td>\n      <td>0.278125</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.376500</td>\n      <td>1.381309</td>\n      <td>0.239063</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.372000</td>\n      <td>1.374159</td>\n      <td>0.425000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.373100</td>\n      <td>1.372520</td>\n      <td>0.239063</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.369500</td>\n      <td>1.367138</td>\n      <td>0.306250</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.363400</td>\n      <td>1.363329</td>\n      <td>0.429688</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.366300</td>\n      <td>1.363788</td>\n      <td>0.325000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.360100</td>\n      <td>1.363522</td>\n      <td>0.393750</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.358900</td>\n      <td>1.361683</td>\n      <td>0.243750</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.355000</td>\n      <td>1.357895</td>\n      <td>0.653125</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.354400</td>\n      <td>1.355133</td>\n      <td>0.632812</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.354700</td>\n      <td>1.353626</td>\n      <td>0.639062</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.354300</td>\n      <td>1.352797</td>\n      <td>0.660937</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.355600</td>\n      <td>1.352938</td>\n      <td>0.690625</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.353700</td>\n      <td>1.352934</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.354000</td>\n      <td>1.352936</td>\n      <td>0.701562</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2000'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:38:45.899741Z","iopub.execute_input":"2025-04-11T17:38:45.900062Z","iopub.status.idle":"2025-04-11T17:39:53.388070Z","shell.execute_reply.started":"2025-04-11T17:38:45.900036Z","shell.execute_reply":"2025-04-11T17:39:53.387186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e35ff9e9e714474bc587d39c32a5b49"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.21it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2000.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define paths\ncsv_path = \"/kaggle/working/saved_models/checkpoint-2000/log_history.csv\"\noutput_dir = \"/kaggle/working/processed_data\"\noutput_csv = os.path.join(output_dir, \"processed_log_history.csv\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read the csv file\ndf = pd.read_csv(csv_path)\n\n# Select desired columns and reorder the dataframe\ndesired_order = [\n    \"step\",         \n    \"loss\",         \n    \"eval_loss\",   \n    \"accuracy\",     \n    \"eval_accuracy\",\n    \"learning_rate\",\n    \"epoch\"         \n]\ndf = df[desired_order]\n\n# Rename columns for uniformity\ndf.rename(columns={\n    \"step\": \"Step\",\n    \"loss\": \"Train Loss\",\n    \"eval_loss\": \"Test Loss\",\n    \"accuracy\": \"Train Acc\",\n    \"eval_accuracy\": \"Test Acc\",\n    \"learning_rate\": \"Learning Rate\",\n    \"epoch\": \"Epochs\"\n}, inplace=True)\n\n# Adding loss spread and loss ratio columns\ndf[\"loss spread\"] = df[\"Train Loss\"] - df[\"Test Loss\"]\ndf[\"loss ratio\"] = df[\"Train Loss\"] / df[\"Test Loss\"]\n\n# Adding acc spread and acc ratio columns\ndf[\"Acc spread\"] = df[\"Test Acc\"] - df[\"Train Acc\"]\ndf[\"Acc ratio\"] = df[\"Test Acc\"] / df[\"Train Acc\"]\n\n# Export the DataFrame as a csv file\ndf.to_csv(output_csv, index=False)\n\nprint(f\"Data exported to {output_csv}\")\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:39:57.405380Z","iopub.execute_input":"2025-04-11T17:39:57.405723Z","iopub.status.idle":"2025-04-11T17:39:57.435504Z","shell.execute_reply.started":"2025-04-11T17:39:57.405694Z","shell.execute_reply":"2025-04-11T17:39:57.434414Z"}},"outputs":[{"name":"stdout","text":"Data exported to /kaggle/working/processed_data/processed_log_history.csv\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3889   1.386846   0.254375  0.226562       0.004969  0.013405   \n1    200      1.3854   1.380215   0.268304  0.287500       0.004878  0.026810   \n2    300      1.3835   1.378506   0.286161  0.418750       0.004728  0.040214   \n3    400      1.3815   1.384044   0.317411  0.259375       0.004523  0.053619   \n4    500      1.3827   1.375127   0.278125  0.278125       0.004268  0.067024   \n5    600      1.3765   1.381309   0.291518  0.239063       0.003969  0.080429   \n6    700      1.3720   1.374159   0.289732  0.425000       0.003635  0.093834   \n7    800      1.3731   1.372520   0.351339  0.239063       0.003273  0.107239   \n8    900      1.3695   1.367138   0.301339  0.306250       0.002891  0.120643   \n9   1000      1.3634   1.363329   0.376786  0.429688       0.002500  0.134048   \n10  1100      1.3663   1.363788   0.368304  0.325000       0.002109  0.147453   \n11  1200      1.3601   1.363522   0.361161  0.393750       0.001727  0.160858   \n12  1300      1.3589   1.361683   0.394643  0.243750       0.001365  0.174263   \n13  1400      1.3550   1.357895   0.374107  0.653125       0.001031  0.187668   \n14  1500      1.3544   1.355133   0.517857  0.632812       0.000732  0.201072   \n15  1600      1.3547   1.353626   0.497321  0.639062       0.000477  0.214477   \n16  1700      1.3543   1.352797   0.497768  0.660937       0.000272  0.227882   \n17  1800      1.3556   1.352938   0.498661  0.690625       0.000122  0.241287   \n18  1900      1.3537   1.352934   0.546875  0.700000       0.000031  0.254692   \n19  2000      1.3540   1.352936   0.528125  0.701562       0.000000  0.268097   \n\n    loss spread  loss ratio  Acc spread  Acc ratio  \n0      0.002054    1.001481   -0.027813   0.890663  \n1      0.005185    1.003757    0.019196   1.071547  \n2      0.004994    1.003623    0.132589   1.463339  \n3     -0.002544    0.998162   -0.058036   0.817159  \n4      0.007573    1.005507    0.000000   1.000000  \n5     -0.004809    0.996519   -0.052455   0.820061  \n6     -0.002159    0.998429    0.135268   1.466872  \n7      0.000580    1.000422   -0.112277   0.680432  \n8      0.002362    1.001728    0.004911   1.016296  \n9      0.000071    1.000052    0.052902   1.140403  \n10     0.002512    1.001842   -0.043304   0.882424  \n11    -0.003422    0.997491    0.032589   1.090235  \n12    -0.002783    0.997956   -0.150893   0.617647  \n13    -0.002895    0.997868    0.279018   1.745823  \n14    -0.000733    0.999459    0.114955   1.221983  \n15     0.001074    1.000793    0.141741   1.285009  \n16     0.001503    1.001111    0.163170   1.327803  \n17     0.002662    1.001968    0.191964   1.384960  \n18     0.000766    1.000566    0.153125   1.280000  \n19     0.001064    1.000786    0.173438   1.328402  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>loss spread</th>\n      <th>loss ratio</th>\n      <th>Acc spread</th>\n      <th>Acc ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3889</td>\n      <td>1.386846</td>\n      <td>0.254375</td>\n      <td>0.226562</td>\n      <td>0.004969</td>\n      <td>0.013405</td>\n      <td>0.002054</td>\n      <td>1.001481</td>\n      <td>-0.027813</td>\n      <td>0.890663</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1.3854</td>\n      <td>1.380215</td>\n      <td>0.268304</td>\n      <td>0.287500</td>\n      <td>0.004878</td>\n      <td>0.026810</td>\n      <td>0.005185</td>\n      <td>1.003757</td>\n      <td>0.019196</td>\n      <td>1.071547</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>1.3835</td>\n      <td>1.378506</td>\n      <td>0.286161</td>\n      <td>0.418750</td>\n      <td>0.004728</td>\n      <td>0.040214</td>\n      <td>0.004994</td>\n      <td>1.003623</td>\n      <td>0.132589</td>\n      <td>1.463339</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>1.3815</td>\n      <td>1.384044</td>\n      <td>0.317411</td>\n      <td>0.259375</td>\n      <td>0.004523</td>\n      <td>0.053619</td>\n      <td>-0.002544</td>\n      <td>0.998162</td>\n      <td>-0.058036</td>\n      <td>0.817159</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>1.3827</td>\n      <td>1.375127</td>\n      <td>0.278125</td>\n      <td>0.278125</td>\n      <td>0.004268</td>\n      <td>0.067024</td>\n      <td>0.007573</td>\n      <td>1.005507</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>1.3765</td>\n      <td>1.381309</td>\n      <td>0.291518</td>\n      <td>0.239063</td>\n      <td>0.003969</td>\n      <td>0.080429</td>\n      <td>-0.004809</td>\n      <td>0.996519</td>\n      <td>-0.052455</td>\n      <td>0.820061</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>1.3720</td>\n      <td>1.374159</td>\n      <td>0.289732</td>\n      <td>0.425000</td>\n      <td>0.003635</td>\n      <td>0.093834</td>\n      <td>-0.002159</td>\n      <td>0.998429</td>\n      <td>0.135268</td>\n      <td>1.466872</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>1.3731</td>\n      <td>1.372520</td>\n      <td>0.351339</td>\n      <td>0.239063</td>\n      <td>0.003273</td>\n      <td>0.107239</td>\n      <td>0.000580</td>\n      <td>1.000422</td>\n      <td>-0.112277</td>\n      <td>0.680432</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>1.3695</td>\n      <td>1.367138</td>\n      <td>0.301339</td>\n      <td>0.306250</td>\n      <td>0.002891</td>\n      <td>0.120643</td>\n      <td>0.002362</td>\n      <td>1.001728</td>\n      <td>0.004911</td>\n      <td>1.016296</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>1.3634</td>\n      <td>1.363329</td>\n      <td>0.376786</td>\n      <td>0.429688</td>\n      <td>0.002500</td>\n      <td>0.134048</td>\n      <td>0.000071</td>\n      <td>1.000052</td>\n      <td>0.052902</td>\n      <td>1.140403</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>1.3663</td>\n      <td>1.363788</td>\n      <td>0.368304</td>\n      <td>0.325000</td>\n      <td>0.002109</td>\n      <td>0.147453</td>\n      <td>0.002512</td>\n      <td>1.001842</td>\n      <td>-0.043304</td>\n      <td>0.882424</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>1.3601</td>\n      <td>1.363522</td>\n      <td>0.361161</td>\n      <td>0.393750</td>\n      <td>0.001727</td>\n      <td>0.160858</td>\n      <td>-0.003422</td>\n      <td>0.997491</td>\n      <td>0.032589</td>\n      <td>1.090235</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>1.3589</td>\n      <td>1.361683</td>\n      <td>0.394643</td>\n      <td>0.243750</td>\n      <td>0.001365</td>\n      <td>0.174263</td>\n      <td>-0.002783</td>\n      <td>0.997956</td>\n      <td>-0.150893</td>\n      <td>0.617647</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>1.3550</td>\n      <td>1.357895</td>\n      <td>0.374107</td>\n      <td>0.653125</td>\n      <td>0.001031</td>\n      <td>0.187668</td>\n      <td>-0.002895</td>\n      <td>0.997868</td>\n      <td>0.279018</td>\n      <td>1.745823</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>1.3544</td>\n      <td>1.355133</td>\n      <td>0.517857</td>\n      <td>0.632812</td>\n      <td>0.000732</td>\n      <td>0.201072</td>\n      <td>-0.000733</td>\n      <td>0.999459</td>\n      <td>0.114955</td>\n      <td>1.221983</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>1.3547</td>\n      <td>1.353626</td>\n      <td>0.497321</td>\n      <td>0.639062</td>\n      <td>0.000477</td>\n      <td>0.214477</td>\n      <td>0.001074</td>\n      <td>1.000793</td>\n      <td>0.141741</td>\n      <td>1.285009</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>1.3543</td>\n      <td>1.352797</td>\n      <td>0.497768</td>\n      <td>0.660937</td>\n      <td>0.000272</td>\n      <td>0.227882</td>\n      <td>0.001503</td>\n      <td>1.001111</td>\n      <td>0.163170</td>\n      <td>1.327803</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>1.3556</td>\n      <td>1.352938</td>\n      <td>0.498661</td>\n      <td>0.690625</td>\n      <td>0.000122</td>\n      <td>0.241287</td>\n      <td>0.002662</td>\n      <td>1.001968</td>\n      <td>0.191964</td>\n      <td>1.384960</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>1.3537</td>\n      <td>1.352934</td>\n      <td>0.546875</td>\n      <td>0.700000</td>\n      <td>0.000031</td>\n      <td>0.254692</td>\n      <td>0.000766</td>\n      <td>1.000566</td>\n      <td>0.153125</td>\n      <td>1.280000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>1.3540</td>\n      <td>1.352936</td>\n      <td>0.528125</td>\n      <td>0.701562</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.001064</td>\n      <td>1.000786</td>\n      <td>0.173438</td>\n      <td>1.328402</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30}]}