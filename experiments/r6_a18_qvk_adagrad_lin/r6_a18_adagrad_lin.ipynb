{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"},{"sourceId":11368871,"sourceType":"datasetVersion","datasetId":7116750}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:13:29.241617Z","iopub.execute_input":"2025-04-11T14:13:29.241810Z","iopub.status.idle":"2025-04-11T14:13:30.417888Z","shell.execute_reply.started":"2025-04-11T14:13:29.241791Z","shell.execute_reply":"2025-04-11T14:13:30.416649Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 27, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (19/19), done.\u001b[K\nremote: Total 27 (delta 10), reused 23 (delta 6), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (27/27), 6.89 KiB | 3.45 MiB/s, done.\nResolving deltas: 100% (10/10), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=6,\n    lora_alpha=18,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query','key','value'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:13:33.923220Z","iopub.execute_input":"2025-04-11T14:13:33.923554Z","iopub.status.idle":"2025-04-11T14:13:56.440016Z","shell.execute_reply.started":"2025-04-11T14:13:33.923527Z","shell.execute_reply":"2025-04-11T14:13:56.439077Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633d36d52cf742c5bdd27f724f3fe0f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f5588b71d541d9864f56bb98c02705"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T19:13:49.129893Z","iopub.execute_input":"2025-04-10T19:13:49.130169Z","iopub.status.idle":"2025-04-10T19:13:49.138913Z","shell.execute_reply.started":"2025-04-10T19:13:49.130149Z","shell.execute_reply":"2025-04-10T19:13:49.138049Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=6, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=6, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adagrad', \n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:14:04.304701Z","iopub.execute_input":"2025-04-11T14:14:04.305330Z","iopub.status.idle":"2025-04-11T14:32:13.184238Z","shell.execute_reply.started":"2025-04-11T14:14:04.305300Z","shell.execute_reply":"2025-04-11T14:32:13.183068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279c750cb9fe4c6aa9b8e51484c86b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02220139750940439dc3472b52a52a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56bd38951154cb4affe4e0af1b49a5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039102ee1ea04e52b5ae93faa2404488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d1a31ce416492386d7ec5affb288a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37eb764917804821bbe1deb8c78600f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba21227fe2644eda8d03ed87a033a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5910fc82dcb04bdbb7ac67866060425f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c930abfe42f7473f862123ee76c52ad4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93692f03f16544648127d5631476c821"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 16:49, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.387600</td>\n      <td>1.375613</td>\n      <td>0.296875</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.379200</td>\n      <td>1.372357</td>\n      <td>0.317188</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.373300</td>\n      <td>1.369072</td>\n      <td>0.337500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.367900</td>\n      <td>1.365985</td>\n      <td>0.354687</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.369700</td>\n      <td>1.362820</td>\n      <td>0.354687</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.366300</td>\n      <td>1.359850</td>\n      <td>0.415625</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.361900</td>\n      <td>1.356608</td>\n      <td>0.426563</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.359800</td>\n      <td>1.354006</td>\n      <td>0.465625</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.355900</td>\n      <td>1.351321</td>\n      <td>0.515625</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.353700</td>\n      <td>1.348671</td>\n      <td>0.554688</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.350900</td>\n      <td>1.346593</td>\n      <td>0.635938</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.345400</td>\n      <td>1.344482</td>\n      <td>0.692187</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.350300</td>\n      <td>1.342618</td>\n      <td>0.731250</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.343700</td>\n      <td>1.340901</td>\n      <td>0.739062</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.343700</td>\n      <td>1.339328</td>\n      <td>0.756250</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.342600</td>\n      <td>1.338049</td>\n      <td>0.767188</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.338500</td>\n      <td>1.337126</td>\n      <td>0.770312</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.339800</td>\n      <td>1.336469</td>\n      <td>0.775000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.340000</td>\n      <td>1.336098</td>\n      <td>0.778125</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.337600</td>\n      <td>1.335982</td>\n      <td>0.778125</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2000'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T14:32:13.186306Z","iopub.execute_input":"2025-04-11T14:32:13.186560Z","iopub.status.idle":"2025-04-11T14:33:20.414493Z","shell.execute_reply.started":"2025-04-11T14:32:13.186535Z","shell.execute_reply":"2025-04-11T14:33:20.413681Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c4f0f3a0fb4202a7f8620278622b8a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.22it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2000.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n\n# Define the paths\ncsv_path = \"/kaggle/working/saved_models/checkpoint-2000/log_history.csv\"\noutput_dir = \"/kaggle/working/processed_data\"\noutput_csv = os.path.join(output_dir, \"processed_log_history.csv\")\n\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read the csv file\ndf = pd.read_csv(csv_path)\n\n# Select desired columns and reorder the dataframe\ndesired_order = [\n    \"step\",         \n    \"loss\",         \n    \"eval_loss\",   \n    \"accuracy\",     \n    \"eval_accuracy\",\n    \"learning_rate\",\n    \"epoch\"         \n]\ndf = df[desired_order]\n\n# Rename columns for uniformity\ndf.rename(columns={\n    \"step\": \"Step\",\n    \"loss\": \"Train Loss\",\n    \"eval_loss\": \"Test Loss\",\n    \"accuracy\": \"Train Acc\",\n    \"eval_accuracy\": \"Test Acc\",\n    \"learning_rate\": \"Learning Rate\",\n    \"epoch\": \"Epochs\"\n}, inplace=True)\n\n# Adding loss spread and loss ratio columns\ndf[\"loss spread\"] = df[\"Train Loss\"] - df[\"Test Loss\"]\ndf[\"loss ratio\"] = df[\"Train Loss\"] / df[\"Test Loss\"]\n\n# Adding acc spread and acc ratio columns\ndf[\"Acc spread\"] = df[\"Test Acc\"] - df[\"Train Acc\"]\ndf[\"Acc ratio\"] = df[\"Test Acc\"] / df[\"Train Acc\"]\n\n# Export the DataFrame as a csv file\ndf.to_csv(output_csv, index=False)\n\nprint(f\"Data exported to {output_csv}\")\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T15:25:12.186978Z","iopub.execute_input":"2025-04-11T15:25:12.187351Z","iopub.status.idle":"2025-04-11T15:25:12.215073Z","shell.execute_reply.started":"2025-04-11T15:25:12.187321Z","shell.execute_reply":"2025-04-11T15:25:12.214261Z"}},"outputs":[{"name":"stdout","text":"Data exported to /kaggle/working/processed_data/processed_log_history.csv\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3876   1.375613   0.281875  0.296875       0.000048  0.013405   \n1    200      1.3792   1.372357   0.287054  0.317188       0.000045  0.026810   \n2    300      1.3733   1.369072   0.315179  0.337500       0.000043  0.040214   \n3    400      1.3679   1.365985   0.347321  0.354687       0.000040  0.053619   \n4    500      1.3697   1.362820   0.341518  0.354687       0.000037  0.067024   \n5    600      1.3663   1.359850   0.358482  0.415625       0.000035  0.080429   \n6    700      1.3619   1.356608   0.401339  0.426563       0.000033  0.093834   \n7    800      1.3598   1.354006   0.411161  0.465625       0.000030  0.107239   \n8    900      1.3559   1.351321   0.436607  0.515625       0.000028  0.120643   \n9   1000      1.3537   1.348671   0.461607  0.554688       0.000025  0.134048   \n10  1100      1.3509   1.346593   0.477679  0.635938       0.000023  0.147453   \n11  1200      1.3454   1.344482   0.525893  0.692187       0.000020  0.160858   \n12  1300      1.3503   1.342618   0.516964  0.731250       0.000017  0.174263   \n13  1400      1.3437   1.340901   0.552232  0.739062       0.000015  0.187668   \n14  1500      1.3437   1.339328   0.570536  0.756250       0.000013  0.201072   \n15  1600      1.3426   1.338049   0.578571  0.767188       0.000010  0.214477   \n16  1700      1.3385   1.337126   0.601339  0.770312       0.000008  0.227882   \n17  1800      1.3398   1.336469   0.590179  0.775000       0.000005  0.241287   \n18  1900      1.3400   1.336098   0.602679  0.778125       0.000003  0.254692   \n19  2000      1.3376   1.335982   0.611161  0.778125       0.000000  0.268097   \n\n    loss spread  loss ratio  Acc spread  Acc ratio  \n0      0.011987    1.008714    0.015000   1.053215  \n1      0.006843    1.004986    0.030134   1.104977  \n2      0.004228    1.003088    0.022321   1.070822  \n3      0.001915    1.001402    0.007366   1.021208  \n4      0.006880    1.005049    0.013170   1.038562  \n5      0.006450    1.004743    0.057143   1.159402  \n6      0.005292    1.003901    0.025223   1.062848  \n7      0.005794    1.004279    0.054464   1.132465  \n8      0.004579    1.003388    0.079018   1.180982  \n9      0.005029    1.003729    0.093080   1.201644  \n10     0.004307    1.003198    0.158259   1.331308  \n11     0.000918    1.000683    0.166295   1.316214  \n12     0.007682    1.005721    0.214286   1.414508  \n13     0.002799    1.002087    0.186830   1.338319  \n14     0.004372    1.003264    0.185714   1.325509  \n15     0.004551    1.003401    0.188616   1.326003  \n16     0.001374    1.001028    0.168973   1.280995  \n17     0.003331    1.002493    0.184821   1.313162  \n18     0.003902    1.002920    0.175446   1.291111  \n19     0.001618    1.001211    0.166964   1.273192  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>loss spread</th>\n      <th>loss ratio</th>\n      <th>Acc spread</th>\n      <th>Acc ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3876</td>\n      <td>1.375613</td>\n      <td>0.281875</td>\n      <td>0.296875</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.011987</td>\n      <td>1.008714</td>\n      <td>0.015000</td>\n      <td>1.053215</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1.3792</td>\n      <td>1.372357</td>\n      <td>0.287054</td>\n      <td>0.317188</td>\n      <td>0.000045</td>\n      <td>0.026810</td>\n      <td>0.006843</td>\n      <td>1.004986</td>\n      <td>0.030134</td>\n      <td>1.104977</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>1.3733</td>\n      <td>1.369072</td>\n      <td>0.315179</td>\n      <td>0.337500</td>\n      <td>0.000043</td>\n      <td>0.040214</td>\n      <td>0.004228</td>\n      <td>1.003088</td>\n      <td>0.022321</td>\n      <td>1.070822</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>1.3679</td>\n      <td>1.365985</td>\n      <td>0.347321</td>\n      <td>0.354687</td>\n      <td>0.000040</td>\n      <td>0.053619</td>\n      <td>0.001915</td>\n      <td>1.001402</td>\n      <td>0.007366</td>\n      <td>1.021208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>1.3697</td>\n      <td>1.362820</td>\n      <td>0.341518</td>\n      <td>0.354687</td>\n      <td>0.000037</td>\n      <td>0.067024</td>\n      <td>0.006880</td>\n      <td>1.005049</td>\n      <td>0.013170</td>\n      <td>1.038562</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>1.3663</td>\n      <td>1.359850</td>\n      <td>0.358482</td>\n      <td>0.415625</td>\n      <td>0.000035</td>\n      <td>0.080429</td>\n      <td>0.006450</td>\n      <td>1.004743</td>\n      <td>0.057143</td>\n      <td>1.159402</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>1.3619</td>\n      <td>1.356608</td>\n      <td>0.401339</td>\n      <td>0.426563</td>\n      <td>0.000033</td>\n      <td>0.093834</td>\n      <td>0.005292</td>\n      <td>1.003901</td>\n      <td>0.025223</td>\n      <td>1.062848</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>1.3598</td>\n      <td>1.354006</td>\n      <td>0.411161</td>\n      <td>0.465625</td>\n      <td>0.000030</td>\n      <td>0.107239</td>\n      <td>0.005794</td>\n      <td>1.004279</td>\n      <td>0.054464</td>\n      <td>1.132465</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>1.3559</td>\n      <td>1.351321</td>\n      <td>0.436607</td>\n      <td>0.515625</td>\n      <td>0.000028</td>\n      <td>0.120643</td>\n      <td>0.004579</td>\n      <td>1.003388</td>\n      <td>0.079018</td>\n      <td>1.180982</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>1.3537</td>\n      <td>1.348671</td>\n      <td>0.461607</td>\n      <td>0.554688</td>\n      <td>0.000025</td>\n      <td>0.134048</td>\n      <td>0.005029</td>\n      <td>1.003729</td>\n      <td>0.093080</td>\n      <td>1.201644</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>1.3509</td>\n      <td>1.346593</td>\n      <td>0.477679</td>\n      <td>0.635938</td>\n      <td>0.000023</td>\n      <td>0.147453</td>\n      <td>0.004307</td>\n      <td>1.003198</td>\n      <td>0.158259</td>\n      <td>1.331308</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>1.3454</td>\n      <td>1.344482</td>\n      <td>0.525893</td>\n      <td>0.692187</td>\n      <td>0.000020</td>\n      <td>0.160858</td>\n      <td>0.000918</td>\n      <td>1.000683</td>\n      <td>0.166295</td>\n      <td>1.316214</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>1.3503</td>\n      <td>1.342618</td>\n      <td>0.516964</td>\n      <td>0.731250</td>\n      <td>0.000017</td>\n      <td>0.174263</td>\n      <td>0.007682</td>\n      <td>1.005721</td>\n      <td>0.214286</td>\n      <td>1.414508</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>1.3437</td>\n      <td>1.340901</td>\n      <td>0.552232</td>\n      <td>0.739062</td>\n      <td>0.000015</td>\n      <td>0.187668</td>\n      <td>0.002799</td>\n      <td>1.002087</td>\n      <td>0.186830</td>\n      <td>1.338319</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>1.3437</td>\n      <td>1.339328</td>\n      <td>0.570536</td>\n      <td>0.756250</td>\n      <td>0.000013</td>\n      <td>0.201072</td>\n      <td>0.004372</td>\n      <td>1.003264</td>\n      <td>0.185714</td>\n      <td>1.325509</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>1.3426</td>\n      <td>1.338049</td>\n      <td>0.578571</td>\n      <td>0.767188</td>\n      <td>0.000010</td>\n      <td>0.214477</td>\n      <td>0.004551</td>\n      <td>1.003401</td>\n      <td>0.188616</td>\n      <td>1.326003</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>1.3385</td>\n      <td>1.337126</td>\n      <td>0.601339</td>\n      <td>0.770312</td>\n      <td>0.000008</td>\n      <td>0.227882</td>\n      <td>0.001374</td>\n      <td>1.001028</td>\n      <td>0.168973</td>\n      <td>1.280995</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>1.3398</td>\n      <td>1.336469</td>\n      <td>0.590179</td>\n      <td>0.775000</td>\n      <td>0.000005</td>\n      <td>0.241287</td>\n      <td>0.003331</td>\n      <td>1.002493</td>\n      <td>0.184821</td>\n      <td>1.313162</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>1.3400</td>\n      <td>1.336098</td>\n      <td>0.602679</td>\n      <td>0.778125</td>\n      <td>0.000003</td>\n      <td>0.254692</td>\n      <td>0.003902</td>\n      <td>1.002920</td>\n      <td>0.175446</td>\n      <td>1.291111</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>1.3376</td>\n      <td>1.335982</td>\n      <td>0.611161</td>\n      <td>0.778125</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.001618</td>\n      <td>1.001211</td>\n      <td>0.166964</td>\n      <td>1.273192</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28}]}