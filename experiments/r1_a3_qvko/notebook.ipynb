{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:39:58.989148Z","iopub.execute_input":"2025-04-14T23:39:58.989398Z","iopub.status.idle":"2025-04-14T23:40:00.252281Z","shell.execute_reply.started":"2025-04-14T23:39:58.989372Z","shell.execute_reply":"2025-04-14T23:40:00.251252Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 352.00 KiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=1,\n    lora_alpha=3,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query', 'key', 'value', 'attention.output.dense'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:40:39.477139Z","iopub.execute_input":"2025-04-14T23:40:39.477463Z","iopub.status.idle":"2025-04-14T23:41:02.303128Z","shell.execute_reply.started":"2025-04-14T23:40:39.477431Z","shell.execute_reply":"2025-04-14T23:41:02.302389Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e2b3f6fcc74be0bf5aa916fc9f5188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3efa6517cf324352b04b8916a5effd24"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 667,396 || all params: 125,316,104 || trainable%: 0.5326\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:41:06.735657Z","iopub.execute_input":"2025-04-14T23:41:06.736238Z","iopub.status.idle":"2025-04-14T23:41:06.745154Z","shell.execute_reply.started":"2025-04-14T23:41:06.736210Z","shell.execute_reply":"2025-04-14T23:41:06.744337Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:41:52.147790Z","iopub.execute_input":"2025-04-14T23:41:52.148103Z","iopub.status.idle":"2025-04-15T00:00:09.215425Z","shell.execute_reply.started":"2025-04-14T23:41:52.148082Z","shell.execute_reply":"2025-04-15T00:00:09.214583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eac3a3013764b4487690eec884e1599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66bf3dc591f4b22a551642ba931c150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2567c3d1b674ac1810ef33f1a2fbfa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ccd814635b548b6a4f0bdf29bf57531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf39979c740b4adfa34388bcb78fed13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c528851e24c4a7da87e03e096f1e4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88a73102c9f41828848747440beb871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39dced846b724a128f7484e9033c7b83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c522876bcd414cb1ea62823f48105b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f732469f253141aa933f09200c514ec4"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 16:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.361300</td>\n      <td>1.318448</td>\n      <td>0.725000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.117800</td>\n      <td>0.606654</td>\n      <td>0.867188</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.445400</td>\n      <td>0.366432</td>\n      <td>0.879687</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.306000</td>\n      <td>0.352242</td>\n      <td>0.889062</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.322700</td>\n      <td>0.333524</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.313500</td>\n      <td>0.331322</td>\n      <td>0.892188</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.315700</td>\n      <td>0.335089</td>\n      <td>0.893750</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.340500</td>\n      <td>0.318846</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.310800</td>\n      <td>0.316771</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.301800</td>\n      <td>0.315560</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.309900</td>\n      <td>0.311983</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.308200</td>\n      <td>0.313028</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.336600</td>\n      <td>0.316750</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.295700</td>\n      <td>0.311597</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.302800</td>\n      <td>0.314380</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.333600</td>\n      <td>0.311150</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.279900</td>\n      <td>0.317809</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.296300</td>\n      <td>0.314842</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.269200</td>\n      <td>0.311747</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.344000</td>\n      <td>0.311590</td>\n      <td>0.901563</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"training_args = TrainingArguments(\n    # Core training configs\n    max_steps=3000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    lr_scheduler_type='linear',\n    learning_rate=5e-5,\n\n    # Logging, evaluation, and checkpointing\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    output_dir='/kaggle/working/saved_models',\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args, checkpoint='/kaggle/working/saved_models/checkpoint-2000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:04:20.792204Z","iopub.execute_input":"2025-04-15T00:04:20.792502Z","iopub.status.idle":"2025-04-15T00:12:58.005199Z","shell.execute_reply.started":"2025-04-15T00:04:20.792479Z","shell.execute_reply":"2025-04-15T00:12:58.004332Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nResuming from checkpoint: /kaggle/working/saved_models/checkpoint-2000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 08:29, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2100</td>\n      <td>0.317100</td>\n      <td>0.311241</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.304800</td>\n      <td>0.309564</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.285500</td>\n      <td>0.310045</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.312700</td>\n      <td>0.311079</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.286200</td>\n      <td>0.308379</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.271100</td>\n      <td>0.308054</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.264500</td>\n      <td>0.311223</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.278300</td>\n      <td>0.307168</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.280700</td>\n      <td>0.308609</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.290300</td>\n      <td>0.308272</td>\n      <td>0.906250</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2500'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:40:35.247475Z","iopub.execute_input":"2025-04-15T00:40:35.247820Z","iopub.status.idle":"2025-04-15T00:41:43.286344Z","shell.execute_reply.started":"2025-04-15T00:40:35.247795Z","shell.execute_reply":"2025-04-15T00:41:43.285626Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9859524ed43a40bf9a19f5f790c26c90"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2500.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define paths\ncsv_path = \"/kaggle/working/saved_models/checkpoint-3000/log_history.csv\"\noutput_dir = \"/kaggle/working/processed_data\"\noutput_csv = os.path.join(output_dir, \"processed_log_history.csv\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read the csv file\ndf = pd.read_csv(csv_path)\n\n# Select desired columns and reorder the dataframe\ndesired_order = [\n    \"step\",         \n    \"loss\",         \n    \"eval_loss\",   \n    \"accuracy\",     \n    \"eval_accuracy\",\n    \"learning_rate\",\n    \"epoch\"         \n]\ndf = df[desired_order]\n\n# Rename columns for uniformity\ndf.rename(columns={\n    \"step\": \"Step\",\n    \"loss\": \"Train Loss\",\n    \"eval_loss\": \"Test Loss\",\n    \"accuracy\": \"Train Acc\",\n    \"eval_accuracy\": \"Test Acc\",\n    \"learning_rate\": \"Learning Rate\",\n    \"epoch\": \"Epochs\"\n}, inplace=True)\n\n# Adding loss spread and loss ratio columns\ndf[\"loss spread\"] = df[\"Train Loss\"] - df[\"Test Loss\"]\ndf[\"loss ratio\"] = df[\"Train Loss\"] / df[\"Test Loss\"]\n\n# Adding acc spread and acc ratio columns\ndf[\"Acc spread\"] = df[\"Test Acc\"] - df[\"Train Acc\"]\ndf[\"Acc ratio\"] = df[\"Test Acc\"] / df[\"Train Acc\"]\n\n# Export the DataFrame as a csv file\ndf.to_csv(output_csv, index=False)\n\nprint(f\"Data exported to {output_csv}\")\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:38:44.408113Z","iopub.execute_input":"2025-04-15T00:38:44.408403Z","iopub.status.idle":"2025-04-15T00:38:44.441386Z","shell.execute_reply.started":"2025-04-15T00:38:44.408382Z","shell.execute_reply":"2025-04-15T00:38:44.440693Z"}},"outputs":[{"name":"stdout","text":"Data exported to /kaggle/working/processed_data/processed_log_history.csv\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3613   1.318448   0.384375  0.725000       0.000048  0.013405   \n1    200      1.1178   0.606654   0.715179  0.867188       0.000045  0.026810   \n2    300      0.4454   0.366432   0.868304  0.879687       0.000043  0.040214   \n3    400      0.3060   0.352242   0.899554  0.889062       0.000040  0.053619   \n4    500      0.3227   0.333524   0.895982  0.896875       0.000037  0.067024   \n5    600      0.3135   0.331322   0.889732  0.892188       0.000035  0.080429   \n6    700      0.3157   0.335089   0.897768  0.893750       0.000033  0.093834   \n7    800      0.3405   0.318846   0.885714  0.896875       0.000030  0.107239   \n8    900      0.3108   0.316771   0.895982  0.901563       0.000028  0.120643   \n9   1000      0.3018   0.315560   0.902679  0.901563       0.000025  0.134048   \n10  1100      0.3099   0.311983   0.893304  0.901563       0.000023  0.147453   \n11  1200      0.3082   0.313028   0.903125  0.906250       0.000020  0.160858   \n12  1300      0.3366   0.316750   0.895536  0.903125       0.000017  0.174263   \n13  1400      0.2957   0.311597   0.900000  0.903125       0.000015  0.187668   \n14  1500      0.3028   0.314380   0.899107  0.903125       0.000013  0.201072   \n15  1600      0.3336   0.311150   0.898214  0.904687       0.000010  0.214477   \n16  1700      0.2799   0.317809   0.911161  0.903125       0.000008  0.227882   \n17  1800      0.2963   0.314842   0.903125  0.904687       0.000005  0.241287   \n18  1900      0.2692   0.311747   0.902679  0.903125       0.000003  0.254692   \n19  2000      0.3440   0.311590   0.893750  0.901563       0.000000  0.268097   \n20  2100      0.3171   0.311241   0.890000  0.904687       0.000015  0.281501   \n21  2200      0.3048   0.309564   0.894643  0.903125       0.000013  0.294906   \n22  2300      0.2855   0.310045   0.902679  0.900000       0.000012  0.308311   \n23  2400      0.3127   0.311079   0.895089  0.904687       0.000010  0.321716   \n24  2500      0.2862   0.308379   0.900893  0.907813       0.000008  0.335121   \n25  2600      0.2711   0.308054   0.910268  0.907813       0.000007  0.348525   \n26  2700      0.2645   0.311223   0.908482  0.907813       0.000005  0.361930   \n27  2800      0.2783   0.307168   0.907143  0.904687       0.000003  0.375335   \n28  2900      0.2807   0.308609   0.906696  0.906250       0.000002  0.388740   \n29  3000      0.2903   0.308272   0.900446  0.906250       0.000000  0.402145   \n\n    loss spread  loss ratio  Acc spread  Acc ratio  \n0      0.042852    1.032502    0.340625   1.886179  \n1      0.511146    1.842565    0.152009   1.212547  \n2      0.078968    1.215505    0.011384   1.013111  \n3     -0.046242    0.868720   -0.010491   0.988337  \n4     -0.010824    0.967546    0.000893   1.000997  \n5     -0.017822    0.946209    0.002455   1.002760  \n6     -0.019389    0.942138   -0.004018   0.995525  \n7      0.021654    1.067915    0.011161   1.012601  \n8     -0.005971    0.981151    0.005580   1.006228  \n9     -0.013760    0.956395   -0.001116   0.998764  \n10    -0.002083    0.993323    0.008259   1.009245  \n11    -0.004828    0.984575    0.003125   1.003460  \n12     0.019850    1.062666    0.007589   1.008475  \n13    -0.015897    0.948983    0.003125   1.003472  \n14    -0.011580    0.963165    0.004018   1.004469  \n15     0.022450    1.072150    0.006473   1.007207  \n16    -0.037909    0.880716   -0.008036   0.991181  \n17    -0.018542    0.941107    0.001563   1.001730  \n18    -0.042547    0.863522    0.000446   1.000495  \n19     0.032410    1.104016    0.007812   1.008741  \n20     0.005859    1.018824    0.014687   1.016503  \n21    -0.004764    0.984611    0.008482   1.009481  \n22    -0.024545    0.920833   -0.002679   0.997033  \n23     0.001621    1.005210    0.009598   1.010723  \n24    -0.022179    0.928079    0.006920   1.007681  \n25    -0.036954    0.880039   -0.002455   0.997303  \n26    -0.046723    0.849872   -0.000670   0.999263  \n27    -0.028868    0.906019   -0.002455   0.997293  \n28    -0.027909    0.909566   -0.000446   0.999508  \n29    -0.017972    0.941699    0.005804   1.006445  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>loss spread</th>\n      <th>loss ratio</th>\n      <th>Acc spread</th>\n      <th>Acc ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3613</td>\n      <td>1.318448</td>\n      <td>0.384375</td>\n      <td>0.725000</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.042852</td>\n      <td>1.032502</td>\n      <td>0.340625</td>\n      <td>1.886179</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1.1178</td>\n      <td>0.606654</td>\n      <td>0.715179</td>\n      <td>0.867188</td>\n      <td>0.000045</td>\n      <td>0.026810</td>\n      <td>0.511146</td>\n      <td>1.842565</td>\n      <td>0.152009</td>\n      <td>1.212547</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.4454</td>\n      <td>0.366432</td>\n      <td>0.868304</td>\n      <td>0.879687</td>\n      <td>0.000043</td>\n      <td>0.040214</td>\n      <td>0.078968</td>\n      <td>1.215505</td>\n      <td>0.011384</td>\n      <td>1.013111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.3060</td>\n      <td>0.352242</td>\n      <td>0.899554</td>\n      <td>0.889062</td>\n      <td>0.000040</td>\n      <td>0.053619</td>\n      <td>-0.046242</td>\n      <td>0.868720</td>\n      <td>-0.010491</td>\n      <td>0.988337</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.3227</td>\n      <td>0.333524</td>\n      <td>0.895982</td>\n      <td>0.896875</td>\n      <td>0.000037</td>\n      <td>0.067024</td>\n      <td>-0.010824</td>\n      <td>0.967546</td>\n      <td>0.000893</td>\n      <td>1.000997</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.3135</td>\n      <td>0.331322</td>\n      <td>0.889732</td>\n      <td>0.892188</td>\n      <td>0.000035</td>\n      <td>0.080429</td>\n      <td>-0.017822</td>\n      <td>0.946209</td>\n      <td>0.002455</td>\n      <td>1.002760</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3157</td>\n      <td>0.335089</td>\n      <td>0.897768</td>\n      <td>0.893750</td>\n      <td>0.000033</td>\n      <td>0.093834</td>\n      <td>-0.019389</td>\n      <td>0.942138</td>\n      <td>-0.004018</td>\n      <td>0.995525</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3405</td>\n      <td>0.318846</td>\n      <td>0.885714</td>\n      <td>0.896875</td>\n      <td>0.000030</td>\n      <td>0.107239</td>\n      <td>0.021654</td>\n      <td>1.067915</td>\n      <td>0.011161</td>\n      <td>1.012601</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.3108</td>\n      <td>0.316771</td>\n      <td>0.895982</td>\n      <td>0.901563</td>\n      <td>0.000028</td>\n      <td>0.120643</td>\n      <td>-0.005971</td>\n      <td>0.981151</td>\n      <td>0.005580</td>\n      <td>1.006228</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.3018</td>\n      <td>0.315560</td>\n      <td>0.902679</td>\n      <td>0.901563</td>\n      <td>0.000025</td>\n      <td>0.134048</td>\n      <td>-0.013760</td>\n      <td>0.956395</td>\n      <td>-0.001116</td>\n      <td>0.998764</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.3099</td>\n      <td>0.311983</td>\n      <td>0.893304</td>\n      <td>0.901563</td>\n      <td>0.000023</td>\n      <td>0.147453</td>\n      <td>-0.002083</td>\n      <td>0.993323</td>\n      <td>0.008259</td>\n      <td>1.009245</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3082</td>\n      <td>0.313028</td>\n      <td>0.903125</td>\n      <td>0.906250</td>\n      <td>0.000020</td>\n      <td>0.160858</td>\n      <td>-0.004828</td>\n      <td>0.984575</td>\n      <td>0.003125</td>\n      <td>1.003460</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3366</td>\n      <td>0.316750</td>\n      <td>0.895536</td>\n      <td>0.903125</td>\n      <td>0.000017</td>\n      <td>0.174263</td>\n      <td>0.019850</td>\n      <td>1.062666</td>\n      <td>0.007589</td>\n      <td>1.008475</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2957</td>\n      <td>0.311597</td>\n      <td>0.900000</td>\n      <td>0.903125</td>\n      <td>0.000015</td>\n      <td>0.187668</td>\n      <td>-0.015897</td>\n      <td>0.948983</td>\n      <td>0.003125</td>\n      <td>1.003472</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.3028</td>\n      <td>0.314380</td>\n      <td>0.899107</td>\n      <td>0.903125</td>\n      <td>0.000013</td>\n      <td>0.201072</td>\n      <td>-0.011580</td>\n      <td>0.963165</td>\n      <td>0.004018</td>\n      <td>1.004469</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3336</td>\n      <td>0.311150</td>\n      <td>0.898214</td>\n      <td>0.904687</td>\n      <td>0.000010</td>\n      <td>0.214477</td>\n      <td>0.022450</td>\n      <td>1.072150</td>\n      <td>0.006473</td>\n      <td>1.007207</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2799</td>\n      <td>0.317809</td>\n      <td>0.911161</td>\n      <td>0.903125</td>\n      <td>0.000008</td>\n      <td>0.227882</td>\n      <td>-0.037909</td>\n      <td>0.880716</td>\n      <td>-0.008036</td>\n      <td>0.991181</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2963</td>\n      <td>0.314842</td>\n      <td>0.903125</td>\n      <td>0.904687</td>\n      <td>0.000005</td>\n      <td>0.241287</td>\n      <td>-0.018542</td>\n      <td>0.941107</td>\n      <td>0.001563</td>\n      <td>1.001730</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2692</td>\n      <td>0.311747</td>\n      <td>0.902679</td>\n      <td>0.903125</td>\n      <td>0.000003</td>\n      <td>0.254692</td>\n      <td>-0.042547</td>\n      <td>0.863522</td>\n      <td>0.000446</td>\n      <td>1.000495</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3440</td>\n      <td>0.311590</td>\n      <td>0.893750</td>\n      <td>0.901563</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.032410</td>\n      <td>1.104016</td>\n      <td>0.007812</td>\n      <td>1.008741</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2100</td>\n      <td>0.3171</td>\n      <td>0.311241</td>\n      <td>0.890000</td>\n      <td>0.904687</td>\n      <td>0.000015</td>\n      <td>0.281501</td>\n      <td>0.005859</td>\n      <td>1.018824</td>\n      <td>0.014687</td>\n      <td>1.016503</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2200</td>\n      <td>0.3048</td>\n      <td>0.309564</td>\n      <td>0.894643</td>\n      <td>0.903125</td>\n      <td>0.000013</td>\n      <td>0.294906</td>\n      <td>-0.004764</td>\n      <td>0.984611</td>\n      <td>0.008482</td>\n      <td>1.009481</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2300</td>\n      <td>0.2855</td>\n      <td>0.310045</td>\n      <td>0.902679</td>\n      <td>0.900000</td>\n      <td>0.000012</td>\n      <td>0.308311</td>\n      <td>-0.024545</td>\n      <td>0.920833</td>\n      <td>-0.002679</td>\n      <td>0.997033</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2400</td>\n      <td>0.3127</td>\n      <td>0.311079</td>\n      <td>0.895089</td>\n      <td>0.904687</td>\n      <td>0.000010</td>\n      <td>0.321716</td>\n      <td>0.001621</td>\n      <td>1.005210</td>\n      <td>0.009598</td>\n      <td>1.010723</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2500</td>\n      <td>0.2862</td>\n      <td>0.308379</td>\n      <td>0.900893</td>\n      <td>0.907813</td>\n      <td>0.000008</td>\n      <td>0.335121</td>\n      <td>-0.022179</td>\n      <td>0.928079</td>\n      <td>0.006920</td>\n      <td>1.007681</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2600</td>\n      <td>0.2711</td>\n      <td>0.308054</td>\n      <td>0.910268</td>\n      <td>0.907813</td>\n      <td>0.000007</td>\n      <td>0.348525</td>\n      <td>-0.036954</td>\n      <td>0.880039</td>\n      <td>-0.002455</td>\n      <td>0.997303</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2700</td>\n      <td>0.2645</td>\n      <td>0.311223</td>\n      <td>0.908482</td>\n      <td>0.907813</td>\n      <td>0.000005</td>\n      <td>0.361930</td>\n      <td>-0.046723</td>\n      <td>0.849872</td>\n      <td>-0.000670</td>\n      <td>0.999263</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2800</td>\n      <td>0.2783</td>\n      <td>0.307168</td>\n      <td>0.907143</td>\n      <td>0.904687</td>\n      <td>0.000003</td>\n      <td>0.375335</td>\n      <td>-0.028868</td>\n      <td>0.906019</td>\n      <td>-0.002455</td>\n      <td>0.997293</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2900</td>\n      <td>0.2807</td>\n      <td>0.308609</td>\n      <td>0.906696</td>\n      <td>0.906250</td>\n      <td>0.000002</td>\n      <td>0.388740</td>\n      <td>-0.027909</td>\n      <td>0.909566</td>\n      <td>-0.000446</td>\n      <td>0.999508</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3000</td>\n      <td>0.2903</td>\n      <td>0.308272</td>\n      <td>0.900446</td>\n      <td>0.906250</td>\n      <td>0.000000</td>\n      <td>0.402145</td>\n      <td>-0.017972</td>\n      <td>0.941699</td>\n      <td>0.005804</td>\n      <td>1.006445</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}