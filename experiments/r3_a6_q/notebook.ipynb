{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:51:24.754527Z","iopub.execute_input":"2025-04-14T13:51:24.754779Z","iopub.status.idle":"2025-04-14T13:51:25.675629Z","shell.execute_reply.started":"2025-04-14T13:51:24.754756Z","shell.execute_reply":"2025-04-14T13:51:25.674442Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 33, done.\u001b[K\nremote: Counting objects: 100% (33/33), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 33 (delta 15), reused 26 (delta 8), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (33/33), 8.20 KiB | 2.73 MiB/s, done.\nResolving deltas: 100% (15/15), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=3,\n    lora_alpha=6,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:51:28.391146Z","iopub.execute_input":"2025-04-14T13:51:28.391446Z","iopub.status.idle":"2025-04-14T13:51:52.082520Z","shell.execute_reply.started":"2025-04-14T13:51:28.391421Z","shell.execute_reply":"2025-04-14T13:51:52.081747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7afe2f3d7b6343e48f26a7f8ce95b900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f857e766573348b185a15d906e3bad62"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 648,964 || all params: 125,297,672 || trainable%: 0.5179\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:51:56.393688Z","iopub.execute_input":"2025-04-14T13:51:56.394268Z","iopub.status.idle":"2025-04-14T13:51:56.401859Z","shell.execute_reply.started":"2025-04-14T13:51:56.394241Z","shell.execute_reply":"2025-04-14T13:51:56.400866Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:52:01.852422Z","iopub.execute_input":"2025-04-14T13:52:01.852763Z","iopub.status.idle":"2025-04-14T14:09:09.155279Z","shell.execute_reply.started":"2025-04-14T13:52:01.852737Z","shell.execute_reply":"2025-04-14T14:09:09.154345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b57af43d81c4243b4cf9c7b36c7db0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4375cea1ecd247d1a17b782af45f31bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6524e8365cc7404e97d67cb6b7b0c40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a3fe0e11e44c9d87ecbbca93afa2c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb35561e4b3b46609807737386e61888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"338a2e6d5a14463bb6d63f241dda52e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827b22f20188427c9a96a61000d3f6c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ab2479c9304914b07a7eb86cf03c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a71a6329f147fdbaa8a19c5d18497b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a9e1e023454578a97732b396a04614"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 15:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.368000</td>\n      <td>1.339653</td>\n      <td>0.676562</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.309900</td>\n      <td>1.260085</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.174100</td>\n      <td>1.054297</td>\n      <td>0.868750</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.859500</td>\n      <td>0.636245</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.539200</td>\n      <td>0.390435</td>\n      <td>0.879687</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.416800</td>\n      <td>0.346394</td>\n      <td>0.882812</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.372900</td>\n      <td>0.330234</td>\n      <td>0.892188</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.384400</td>\n      <td>0.320406</td>\n      <td>0.893750</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.353700</td>\n      <td>0.319391</td>\n      <td>0.895312</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.356300</td>\n      <td>0.314483</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.356600</td>\n      <td>0.311670</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.349000</td>\n      <td>0.312508</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.372200</td>\n      <td>0.312490</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.334000</td>\n      <td>0.312664</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.329400</td>\n      <td>0.311219</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.368600</td>\n      <td>0.308169</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.313200</td>\n      <td>0.309413</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.330900</td>\n      <td>0.310167</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.318400</td>\n      <td>0.309815</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.376600</td>\n      <td>0.309693</td>\n      <td>0.906250</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-1600'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:11:51.623958Z","iopub.execute_input":"2025-04-14T14:11:51.624271Z","iopub.status.idle":"2025-04-14T14:12:56.486312Z","shell.execute_reply.started":"2025-04-14T14:11:51.624249Z","shell.execute_reply":"2025-04-14T14:12:56.485366Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e56b6ea41747698dae7445b53a48c8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:57<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-1600.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define paths\ncsv_path = \"/kaggle/working/saved_models/checkpoint-2000/log_history.csv\"\noutput_dir = \"/kaggle/working/processed_data\"\noutput_csv = os.path.join(output_dir, \"processed_log_history.csv\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Read the csv file\ndf = pd.read_csv(csv_path)\n\n# Select desired columns and reorder the dataframe\ndesired_order = [\n    \"step\",         \n    \"loss\",         \n    \"eval_loss\",   \n    \"accuracy\",     \n    \"eval_accuracy\",\n    \"learning_rate\",\n    \"epoch\"         \n]\ndf = df[desired_order]\n\n# Rename columns for uniformity\ndf.rename(columns={\n    \"step\": \"Step\",\n    \"loss\": \"Train Loss\",\n    \"eval_loss\": \"Test Loss\",\n    \"accuracy\": \"Train Acc\",\n    \"eval_accuracy\": \"Test Acc\",\n    \"learning_rate\": \"Learning Rate\",\n    \"epoch\": \"Epochs\"\n}, inplace=True)\n\n# Adding loss spread and loss ratio columns\ndf[\"loss spread\"] = df[\"Train Loss\"] - df[\"Test Loss\"]\ndf[\"loss ratio\"] = df[\"Train Loss\"] / df[\"Test Loss\"]\n\n# Adding acc spread and acc ratio columns\ndf[\"Acc spread\"] = df[\"Test Acc\"] - df[\"Train Acc\"]\ndf[\"Acc ratio\"] = df[\"Test Acc\"] / df[\"Train Acc\"]\n\n# Export the DataFrame as a csv file\ndf.to_csv(output_csv, index=False)\n\nprint(f\"Data exported to {output_csv}\")\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T14:13:53.584967Z","iopub.execute_input":"2025-04-14T14:13:53.585323Z","iopub.status.idle":"2025-04-14T14:13:53.629323Z","shell.execute_reply.started":"2025-04-14T14:13:53.585295Z","shell.execute_reply":"2025-04-14T14:13:53.628398Z"}},"outputs":[{"name":"stdout","text":"Data exported to /kaggle/working/processed_data/processed_log_history.csv\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3680   1.339653   0.368125  0.676562       0.000048  0.013405   \n1    200      1.3099   1.260085   0.563839  0.800000       0.000045  0.026810   \n2    300      1.1741   1.054297   0.781696  0.868750       0.000043  0.040214   \n3    400      0.8595   0.636245   0.871429  0.875000       0.000040  0.053619   \n4    500      0.5392   0.390435   0.880357  0.879687       0.000037  0.067024   \n5    600      0.4168   0.346394   0.885268  0.882812       0.000035  0.080429   \n6    700      0.3729   0.330234   0.884375  0.892188       0.000033  0.093834   \n7    800      0.3844   0.320406   0.878125  0.893750       0.000030  0.107239   \n8    900      0.3537   0.319391   0.890179  0.895312       0.000028  0.120643   \n9   1000      0.3563   0.314483   0.886161  0.896875       0.000025  0.134048   \n10  1100      0.3566   0.311670   0.886607  0.898438       0.000023  0.147453   \n11  1200      0.3490   0.312508   0.892411  0.898438       0.000020  0.160858   \n12  1300      0.3722   0.312490   0.885714  0.900000       0.000017  0.174263   \n13  1400      0.3340   0.312664   0.898214  0.900000       0.000015  0.187668   \n14  1500      0.3294   0.311219   0.893304  0.903125       0.000013  0.201072   \n15  1600      0.3686   0.308169   0.891964  0.906250       0.000010  0.214477   \n16  1700      0.3132   0.309413   0.904018  0.906250       0.000008  0.227882   \n17  1800      0.3309   0.310167   0.900000  0.904687       0.000005  0.241287   \n18  1900      0.3184   0.309815   0.900893  0.904687       0.000003  0.254692   \n19  2000      0.3766   0.309693   0.887500  0.906250       0.000000  0.268097   \n\n    loss spread  loss ratio  Acc spread  Acc ratio  \n0      0.028347    1.021160    0.308437   1.837861  \n1      0.049815    1.039533    0.236161   1.418844  \n2      0.119803    1.113633    0.087054   1.111365  \n3      0.223255    1.350894    0.003571   1.004098  \n4      0.148765    1.381023   -0.000670   0.999239  \n5      0.070406    1.203255   -0.002455   0.997226  \n6      0.042666    1.129198    0.007812   1.008834  \n7      0.063994    1.199728    0.015625   1.017794  \n8      0.034309    1.107419    0.005134   1.005767  \n9      0.041817    1.132970    0.010714   1.012091  \n10     0.044930    1.144158    0.011830   1.013343  \n11     0.036492    1.116773    0.006027   1.006753  \n12     0.059710    1.191078    0.014286   1.016129  \n13     0.021336    1.068240    0.001786   1.001988  \n14     0.018181    1.058419    0.009821   1.010995  \n15     0.060431    1.196096    0.014286   1.016016  \n16     0.003787    1.012241    0.002232   1.002469  \n17     0.020733    1.066845    0.004687   1.005208  \n18     0.008585    1.027712    0.003795   1.004212  \n19     0.066907    1.216042    0.018750   1.021127  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>loss spread</th>\n      <th>loss ratio</th>\n      <th>Acc spread</th>\n      <th>Acc ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3680</td>\n      <td>1.339653</td>\n      <td>0.368125</td>\n      <td>0.676562</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.028347</td>\n      <td>1.021160</td>\n      <td>0.308437</td>\n      <td>1.837861</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1.3099</td>\n      <td>1.260085</td>\n      <td>0.563839</td>\n      <td>0.800000</td>\n      <td>0.000045</td>\n      <td>0.026810</td>\n      <td>0.049815</td>\n      <td>1.039533</td>\n      <td>0.236161</td>\n      <td>1.418844</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>1.1741</td>\n      <td>1.054297</td>\n      <td>0.781696</td>\n      <td>0.868750</td>\n      <td>0.000043</td>\n      <td>0.040214</td>\n      <td>0.119803</td>\n      <td>1.113633</td>\n      <td>0.087054</td>\n      <td>1.111365</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.8595</td>\n      <td>0.636245</td>\n      <td>0.871429</td>\n      <td>0.875000</td>\n      <td>0.000040</td>\n      <td>0.053619</td>\n      <td>0.223255</td>\n      <td>1.350894</td>\n      <td>0.003571</td>\n      <td>1.004098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.5392</td>\n      <td>0.390435</td>\n      <td>0.880357</td>\n      <td>0.879687</td>\n      <td>0.000037</td>\n      <td>0.067024</td>\n      <td>0.148765</td>\n      <td>1.381023</td>\n      <td>-0.000670</td>\n      <td>0.999239</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.4168</td>\n      <td>0.346394</td>\n      <td>0.885268</td>\n      <td>0.882812</td>\n      <td>0.000035</td>\n      <td>0.080429</td>\n      <td>0.070406</td>\n      <td>1.203255</td>\n      <td>-0.002455</td>\n      <td>0.997226</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3729</td>\n      <td>0.330234</td>\n      <td>0.884375</td>\n      <td>0.892188</td>\n      <td>0.000033</td>\n      <td>0.093834</td>\n      <td>0.042666</td>\n      <td>1.129198</td>\n      <td>0.007812</td>\n      <td>1.008834</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3844</td>\n      <td>0.320406</td>\n      <td>0.878125</td>\n      <td>0.893750</td>\n      <td>0.000030</td>\n      <td>0.107239</td>\n      <td>0.063994</td>\n      <td>1.199728</td>\n      <td>0.015625</td>\n      <td>1.017794</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.3537</td>\n      <td>0.319391</td>\n      <td>0.890179</td>\n      <td>0.895312</td>\n      <td>0.000028</td>\n      <td>0.120643</td>\n      <td>0.034309</td>\n      <td>1.107419</td>\n      <td>0.005134</td>\n      <td>1.005767</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.3563</td>\n      <td>0.314483</td>\n      <td>0.886161</td>\n      <td>0.896875</td>\n      <td>0.000025</td>\n      <td>0.134048</td>\n      <td>0.041817</td>\n      <td>1.132970</td>\n      <td>0.010714</td>\n      <td>1.012091</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.3566</td>\n      <td>0.311670</td>\n      <td>0.886607</td>\n      <td>0.898438</td>\n      <td>0.000023</td>\n      <td>0.147453</td>\n      <td>0.044930</td>\n      <td>1.144158</td>\n      <td>0.011830</td>\n      <td>1.013343</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3490</td>\n      <td>0.312508</td>\n      <td>0.892411</td>\n      <td>0.898438</td>\n      <td>0.000020</td>\n      <td>0.160858</td>\n      <td>0.036492</td>\n      <td>1.116773</td>\n      <td>0.006027</td>\n      <td>1.006753</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3722</td>\n      <td>0.312490</td>\n      <td>0.885714</td>\n      <td>0.900000</td>\n      <td>0.000017</td>\n      <td>0.174263</td>\n      <td>0.059710</td>\n      <td>1.191078</td>\n      <td>0.014286</td>\n      <td>1.016129</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.3340</td>\n      <td>0.312664</td>\n      <td>0.898214</td>\n      <td>0.900000</td>\n      <td>0.000015</td>\n      <td>0.187668</td>\n      <td>0.021336</td>\n      <td>1.068240</td>\n      <td>0.001786</td>\n      <td>1.001988</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.3294</td>\n      <td>0.311219</td>\n      <td>0.893304</td>\n      <td>0.903125</td>\n      <td>0.000013</td>\n      <td>0.201072</td>\n      <td>0.018181</td>\n      <td>1.058419</td>\n      <td>0.009821</td>\n      <td>1.010995</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3686</td>\n      <td>0.308169</td>\n      <td>0.891964</td>\n      <td>0.906250</td>\n      <td>0.000010</td>\n      <td>0.214477</td>\n      <td>0.060431</td>\n      <td>1.196096</td>\n      <td>0.014286</td>\n      <td>1.016016</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.3132</td>\n      <td>0.309413</td>\n      <td>0.904018</td>\n      <td>0.906250</td>\n      <td>0.000008</td>\n      <td>0.227882</td>\n      <td>0.003787</td>\n      <td>1.012241</td>\n      <td>0.002232</td>\n      <td>1.002469</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.3309</td>\n      <td>0.310167</td>\n      <td>0.900000</td>\n      <td>0.904687</td>\n      <td>0.000005</td>\n      <td>0.241287</td>\n      <td>0.020733</td>\n      <td>1.066845</td>\n      <td>0.004687</td>\n      <td>1.005208</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.3184</td>\n      <td>0.309815</td>\n      <td>0.900893</td>\n      <td>0.904687</td>\n      <td>0.000003</td>\n      <td>0.254692</td>\n      <td>0.008585</td>\n      <td>1.027712</td>\n      <td>0.003795</td>\n      <td>1.004212</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3766</td>\n      <td>0.309693</td>\n      <td>0.887500</td>\n      <td>0.906250</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.066907</td>\n      <td>1.216042</td>\n      <td>0.018750</td>\n      <td>1.021127</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7}]}