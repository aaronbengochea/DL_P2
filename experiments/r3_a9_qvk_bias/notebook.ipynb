{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:32:51.830688Z","iopub.execute_input":"2025-04-18T19:32:51.830968Z","iopub.status.idle":"2025-04-18T19:32:53.085503Z","shell.execute_reply.started":"2025-04-18T19:32:51.830947Z","shell.execute_reply":"2025-04-18T19:32:53.084740Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 352.00 KiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=3,\n    lora_alpha=9,\n    lora_dropout=0.1,\n    bias='lora_only',\n    target_modules=['query', 'key', 'value'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:32:56.861631Z","iopub.execute_input":"2025-04-18T19:32:56.861946Z","iopub.status.idle":"2025-04-18T19:33:19.596147Z","shell.execute_reply.started":"2025-04-18T19:32:56.861922Z","shell.execute_reply":"2025-04-18T19:33:19.595435Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe1e1de3bd640be994983ec6b832790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332b7b2bbe41488682614e01effadb00"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 787,204 || all params: 125,408,264 || trainable%: 0.6277\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:33:25.813068Z","iopub.execute_input":"2025-04-18T19:33:25.813659Z","iopub.status.idle":"2025-04-18T19:33:25.822350Z","shell.execute_reply.started":"2025-04-18T19:33:25.813632Z","shell.execute_reply":"2025-04-18T19:33:25.821302Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:33:30.184156Z","iopub.execute_input":"2025-04-18T19:33:30.184496Z","iopub.status.idle":"2025-04-18T19:52:15.609378Z","shell.execute_reply.started":"2025-04-18T19:33:30.184473Z","shell.execute_reply":"2025-04-18T19:52:15.608550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d639aa09f91a4aa1b00011741d1def53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7e0ac0afce48898b5e4ee333a429b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f5b3e5c44a4bc180acbad1a1b39a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c843ace5e84331aa6b1f830c675bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d17a23197cf44aa4969e7fbfdad47ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69da4b6ce3764743a8e42b5e2d496e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3073df78adcd401ab51e32dcb1d53f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73fefd8c5a6f491d9be158cfa0354445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc2ac2eb67e4c0aafb71c13d5073fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d6149c94734192b6841faaa0940465"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 17:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.366100</td>\n      <td>1.316062</td>\n      <td>0.681250</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.873800</td>\n      <td>0.386398</td>\n      <td>0.865625</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.371000</td>\n      <td>0.340972</td>\n      <td>0.887500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.308400</td>\n      <td>0.332698</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.318600</td>\n      <td>0.323757</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.299500</td>\n      <td>0.338370</td>\n      <td>0.892188</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.308700</td>\n      <td>0.320299</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.318100</td>\n      <td>0.310010</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.290500</td>\n      <td>0.313633</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.290400</td>\n      <td>0.308268</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.309900</td>\n      <td>0.305544</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.303200</td>\n      <td>0.307111</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.333200</td>\n      <td>0.308133</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.297100</td>\n      <td>0.304031</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.300600</td>\n      <td>0.302731</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.324500</td>\n      <td>0.300705</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.265100</td>\n      <td>0.307492</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.287800</td>\n      <td>0.305099</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.267800</td>\n      <td>0.303749</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.326900</td>\n      <td>0.302909</td>\n      <td>0.904687</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"training_args = TrainingArguments(\n    # Core training configs\n    max_steps=3000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args, checkpoint='/kaggle/working/saved_models/checkpoint-2000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:55:33.092693Z","iopub.execute_input":"2025-04-18T19:55:33.093012Z","iopub.status.idle":"2025-04-18T20:04:06.347959Z","shell.execute_reply.started":"2025-04-18T19:55:33.092984Z","shell.execute_reply":"2025-04-18T20:04:06.347151Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nResuming from checkpoint: /kaggle/working/saved_models/checkpoint-2000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 08:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2100</td>\n      <td>0.301800</td>\n      <td>0.301213</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.294500</td>\n      <td>0.299012</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.269800</td>\n      <td>0.303094</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.301100</td>\n      <td>0.300189</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.276700</td>\n      <td>0.298121</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.265200</td>\n      <td>0.298179</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.263100</td>\n      <td>0.300208</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.266600</td>\n      <td>0.297671</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.277500</td>\n      <td>0.298599</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.265900</td>\n      <td>0.298503</td>\n      <td>0.907813</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2500'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:04:12.880304Z","iopub.execute_input":"2025-04-18T20:04:12.880633Z","iopub.status.idle":"2025-04-18T20:05:19.797471Z","shell.execute_reply.started":"2025-04-18T20:04:12.880605Z","shell.execute_reply":"2025-04-18T20:05:19.796622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e74811b7a13481d91fe67a473c6cab8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.24it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2500.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = '/kaggle/working/saved_models/checkpoint-3000/processed_log_history.csv'\n\ndf = pd.read_csv(csv_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T20:06:03.645527Z","iopub.execute_input":"2025-04-18T20:06:03.645846Z","iopub.status.idle":"2025-04-18T20:06:03.668132Z","shell.execute_reply.started":"2025-04-18T20:06:03.645821Z","shell.execute_reply":"2025-04-18T20:06:03.667289Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3661   1.316062   0.361250  0.681250       0.000048  0.013405   \n1    200      0.8738   0.386398   0.753571  0.865625       0.000045  0.026810   \n2    300      0.3710   0.340972   0.871875  0.887500       0.000043  0.040214   \n3    400      0.3084   0.332698   0.894643  0.896875       0.000040  0.053619   \n4    500      0.3186   0.323757   0.891518  0.901563       0.000037  0.067024   \n5    600      0.2995   0.338370   0.899554  0.892188       0.000035  0.080429   \n6    700      0.3087   0.320299   0.897768  0.904687       0.000033  0.093834   \n7    800      0.3181   0.310010   0.894196  0.906250       0.000030  0.107239   \n8    900      0.2905   0.313633   0.904464  0.904687       0.000028  0.120643   \n9   1000      0.2904   0.308268   0.901786  0.904687       0.000025  0.134048   \n10  1100      0.3099   0.305544   0.901786  0.907813       0.000023  0.147453   \n11  1200      0.3032   0.307111   0.902232  0.906250       0.000020  0.160858   \n12  1300      0.3332   0.308133   0.897321  0.904687       0.000017  0.174263   \n13  1400      0.2971   0.304031   0.901339  0.904687       0.000015  0.187668   \n14  1500      0.3006   0.302731   0.900893  0.904687       0.000013  0.201072   \n15  1600      0.3245   0.300705   0.895536  0.901563       0.000010  0.214477   \n16  1700      0.2651   0.307492   0.908929  0.907813       0.000008  0.227882   \n17  1800      0.2878   0.305099   0.907589  0.906250       0.000005  0.241287   \n18  1900      0.2678   0.303749   0.911161  0.904687       0.000003  0.254692   \n19  2000      0.3269   0.302909   0.900000  0.904687       0.000000  0.268097   \n20  2100      0.3018   0.301213   0.890625  0.907813       0.000015  0.281501   \n21  2200      0.2945   0.299012   0.901339  0.904687       0.000013  0.294906   \n22  2300      0.2698   0.303094   0.904018  0.901563       0.000012  0.308311   \n23  2400      0.3011   0.300189   0.899554  0.904687       0.000010  0.321716   \n24  2500      0.2767   0.298121   0.899107  0.909375       0.000008  0.335121   \n25  2600      0.2652   0.298179   0.910268  0.909375       0.000007  0.348525   \n26  2700      0.2631   0.300208   0.908482  0.909375       0.000005  0.361930   \n27  2800      0.2666   0.297671   0.910714  0.907813       0.000003  0.375335   \n28  2900      0.2775   0.298599   0.904464  0.909375       0.000002  0.388740   \n29  3000      0.2659   0.298503   0.910268  0.907813       0.000000  0.402145   \n\n    Loss Spread  Loss Ratio  Acc Spread  Acc Ratio  \n0      0.050038    1.038021   -0.320000   0.530275  \n1      0.487402    2.261400   -0.112054   0.870552  \n2      0.030028    1.088066   -0.015625   0.982394  \n3     -0.024298    0.926967   -0.002232   0.997511  \n4     -0.005157    0.984072   -0.010045   0.988859  \n5     -0.038870    0.885125    0.007366   1.008256  \n6     -0.011599    0.963787   -0.006920   0.992351  \n7      0.008090    1.026095   -0.012054   0.986700  \n8     -0.023133    0.926243   -0.000223   0.999753  \n9     -0.017868    0.942038   -0.002902   0.996792  \n10     0.004356    1.014256   -0.006027   0.993361  \n11    -0.003911    0.987265   -0.004018   0.995567  \n12     0.025067    1.081352   -0.007366   0.991858  \n13    -0.006931    0.977204   -0.003348   0.996299  \n14    -0.002131    0.992960   -0.003795   0.995806  \n15     0.023795    1.079131   -0.006027   0.993315  \n16    -0.042392    0.862136    0.001116   1.001229  \n17    -0.017299    0.943300    0.001339   1.001478  \n18    -0.035949    0.881649    0.006473   1.007155  \n19     0.023991    1.079200   -0.004687   0.994819  \n20     0.000587    1.001949   -0.017188   0.981067  \n21    -0.004512    0.984910   -0.003348   0.996299  \n22    -0.033294    0.890152    0.002455   1.002723  \n23     0.000911    1.003036   -0.005134   0.994325  \n24    -0.021421    0.928146   -0.010268   0.988709  \n25    -0.032979    0.889399    0.000893   1.000982  \n26    -0.037108    0.876393   -0.000893   0.999018  \n27    -0.031071    0.895619    0.002902   1.003196  \n28    -0.021099    0.929340   -0.004911   0.994600  \n29    -0.032603    0.890779    0.002455   1.002705  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>Loss Spread</th>\n      <th>Loss Ratio</th>\n      <th>Acc Spread</th>\n      <th>Acc Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3661</td>\n      <td>1.316062</td>\n      <td>0.361250</td>\n      <td>0.681250</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.050038</td>\n      <td>1.038021</td>\n      <td>-0.320000</td>\n      <td>0.530275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.8738</td>\n      <td>0.386398</td>\n      <td>0.753571</td>\n      <td>0.865625</td>\n      <td>0.000045</td>\n      <td>0.026810</td>\n      <td>0.487402</td>\n      <td>2.261400</td>\n      <td>-0.112054</td>\n      <td>0.870552</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.3710</td>\n      <td>0.340972</td>\n      <td>0.871875</td>\n      <td>0.887500</td>\n      <td>0.000043</td>\n      <td>0.040214</td>\n      <td>0.030028</td>\n      <td>1.088066</td>\n      <td>-0.015625</td>\n      <td>0.982394</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.3084</td>\n      <td>0.332698</td>\n      <td>0.894643</td>\n      <td>0.896875</td>\n      <td>0.000040</td>\n      <td>0.053619</td>\n      <td>-0.024298</td>\n      <td>0.926967</td>\n      <td>-0.002232</td>\n      <td>0.997511</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.3186</td>\n      <td>0.323757</td>\n      <td>0.891518</td>\n      <td>0.901563</td>\n      <td>0.000037</td>\n      <td>0.067024</td>\n      <td>-0.005157</td>\n      <td>0.984072</td>\n      <td>-0.010045</td>\n      <td>0.988859</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.2995</td>\n      <td>0.338370</td>\n      <td>0.899554</td>\n      <td>0.892188</td>\n      <td>0.000035</td>\n      <td>0.080429</td>\n      <td>-0.038870</td>\n      <td>0.885125</td>\n      <td>0.007366</td>\n      <td>1.008256</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3087</td>\n      <td>0.320299</td>\n      <td>0.897768</td>\n      <td>0.904687</td>\n      <td>0.000033</td>\n      <td>0.093834</td>\n      <td>-0.011599</td>\n      <td>0.963787</td>\n      <td>-0.006920</td>\n      <td>0.992351</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3181</td>\n      <td>0.310010</td>\n      <td>0.894196</td>\n      <td>0.906250</td>\n      <td>0.000030</td>\n      <td>0.107239</td>\n      <td>0.008090</td>\n      <td>1.026095</td>\n      <td>-0.012054</td>\n      <td>0.986700</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.2905</td>\n      <td>0.313633</td>\n      <td>0.904464</td>\n      <td>0.904687</td>\n      <td>0.000028</td>\n      <td>0.120643</td>\n      <td>-0.023133</td>\n      <td>0.926243</td>\n      <td>-0.000223</td>\n      <td>0.999753</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2904</td>\n      <td>0.308268</td>\n      <td>0.901786</td>\n      <td>0.904687</td>\n      <td>0.000025</td>\n      <td>0.134048</td>\n      <td>-0.017868</td>\n      <td>0.942038</td>\n      <td>-0.002902</td>\n      <td>0.996792</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.3099</td>\n      <td>0.305544</td>\n      <td>0.901786</td>\n      <td>0.907813</td>\n      <td>0.000023</td>\n      <td>0.147453</td>\n      <td>0.004356</td>\n      <td>1.014256</td>\n      <td>-0.006027</td>\n      <td>0.993361</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3032</td>\n      <td>0.307111</td>\n      <td>0.902232</td>\n      <td>0.906250</td>\n      <td>0.000020</td>\n      <td>0.160858</td>\n      <td>-0.003911</td>\n      <td>0.987265</td>\n      <td>-0.004018</td>\n      <td>0.995567</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3332</td>\n      <td>0.308133</td>\n      <td>0.897321</td>\n      <td>0.904687</td>\n      <td>0.000017</td>\n      <td>0.174263</td>\n      <td>0.025067</td>\n      <td>1.081352</td>\n      <td>-0.007366</td>\n      <td>0.991858</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2971</td>\n      <td>0.304031</td>\n      <td>0.901339</td>\n      <td>0.904687</td>\n      <td>0.000015</td>\n      <td>0.187668</td>\n      <td>-0.006931</td>\n      <td>0.977204</td>\n      <td>-0.003348</td>\n      <td>0.996299</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.3006</td>\n      <td>0.302731</td>\n      <td>0.900893</td>\n      <td>0.904687</td>\n      <td>0.000013</td>\n      <td>0.201072</td>\n      <td>-0.002131</td>\n      <td>0.992960</td>\n      <td>-0.003795</td>\n      <td>0.995806</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3245</td>\n      <td>0.300705</td>\n      <td>0.895536</td>\n      <td>0.901563</td>\n      <td>0.000010</td>\n      <td>0.214477</td>\n      <td>0.023795</td>\n      <td>1.079131</td>\n      <td>-0.006027</td>\n      <td>0.993315</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2651</td>\n      <td>0.307492</td>\n      <td>0.908929</td>\n      <td>0.907813</td>\n      <td>0.000008</td>\n      <td>0.227882</td>\n      <td>-0.042392</td>\n      <td>0.862136</td>\n      <td>0.001116</td>\n      <td>1.001229</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2878</td>\n      <td>0.305099</td>\n      <td>0.907589</td>\n      <td>0.906250</td>\n      <td>0.000005</td>\n      <td>0.241287</td>\n      <td>-0.017299</td>\n      <td>0.943300</td>\n      <td>0.001339</td>\n      <td>1.001478</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2678</td>\n      <td>0.303749</td>\n      <td>0.911161</td>\n      <td>0.904687</td>\n      <td>0.000003</td>\n      <td>0.254692</td>\n      <td>-0.035949</td>\n      <td>0.881649</td>\n      <td>0.006473</td>\n      <td>1.007155</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3269</td>\n      <td>0.302909</td>\n      <td>0.900000</td>\n      <td>0.904687</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.023991</td>\n      <td>1.079200</td>\n      <td>-0.004687</td>\n      <td>0.994819</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2100</td>\n      <td>0.3018</td>\n      <td>0.301213</td>\n      <td>0.890625</td>\n      <td>0.907813</td>\n      <td>0.000015</td>\n      <td>0.281501</td>\n      <td>0.000587</td>\n      <td>1.001949</td>\n      <td>-0.017188</td>\n      <td>0.981067</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2200</td>\n      <td>0.2945</td>\n      <td>0.299012</td>\n      <td>0.901339</td>\n      <td>0.904687</td>\n      <td>0.000013</td>\n      <td>0.294906</td>\n      <td>-0.004512</td>\n      <td>0.984910</td>\n      <td>-0.003348</td>\n      <td>0.996299</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2300</td>\n      <td>0.2698</td>\n      <td>0.303094</td>\n      <td>0.904018</td>\n      <td>0.901563</td>\n      <td>0.000012</td>\n      <td>0.308311</td>\n      <td>-0.033294</td>\n      <td>0.890152</td>\n      <td>0.002455</td>\n      <td>1.002723</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2400</td>\n      <td>0.3011</td>\n      <td>0.300189</td>\n      <td>0.899554</td>\n      <td>0.904687</td>\n      <td>0.000010</td>\n      <td>0.321716</td>\n      <td>0.000911</td>\n      <td>1.003036</td>\n      <td>-0.005134</td>\n      <td>0.994325</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2500</td>\n      <td>0.2767</td>\n      <td>0.298121</td>\n      <td>0.899107</td>\n      <td>0.909375</td>\n      <td>0.000008</td>\n      <td>0.335121</td>\n      <td>-0.021421</td>\n      <td>0.928146</td>\n      <td>-0.010268</td>\n      <td>0.988709</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2600</td>\n      <td>0.2652</td>\n      <td>0.298179</td>\n      <td>0.910268</td>\n      <td>0.909375</td>\n      <td>0.000007</td>\n      <td>0.348525</td>\n      <td>-0.032979</td>\n      <td>0.889399</td>\n      <td>0.000893</td>\n      <td>1.000982</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2700</td>\n      <td>0.2631</td>\n      <td>0.300208</td>\n      <td>0.908482</td>\n      <td>0.909375</td>\n      <td>0.000005</td>\n      <td>0.361930</td>\n      <td>-0.037108</td>\n      <td>0.876393</td>\n      <td>-0.000893</td>\n      <td>0.999018</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2800</td>\n      <td>0.2666</td>\n      <td>0.297671</td>\n      <td>0.910714</td>\n      <td>0.907813</td>\n      <td>0.000003</td>\n      <td>0.375335</td>\n      <td>-0.031071</td>\n      <td>0.895619</td>\n      <td>0.002902</td>\n      <td>1.003196</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2900</td>\n      <td>0.2775</td>\n      <td>0.298599</td>\n      <td>0.904464</td>\n      <td>0.909375</td>\n      <td>0.000002</td>\n      <td>0.388740</td>\n      <td>-0.021099</td>\n      <td>0.929340</td>\n      <td>-0.004911</td>\n      <td>0.994600</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3000</td>\n      <td>0.2659</td>\n      <td>0.298503</td>\n      <td>0.910268</td>\n      <td>0.907813</td>\n      <td>0.000000</td>\n      <td>0.402145</td>\n      <td>-0.032603</td>\n      <td>0.890779</td>\n      <td>0.002455</td>\n      <td>1.002705</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}