{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:41:32.125603Z","iopub.execute_input":"2025-04-17T00:41:32.125871Z","iopub.status.idle":"2025-04-17T00:41:32.863464Z","shell.execute_reply.started":"2025-04-17T00:41:32.125844Z","shell.execute_reply":"2025-04-17T00:41:32.862596Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 3.56 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=1,\n    lora_alpha=2,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query', 'key', 'value'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:41:37.345516Z","iopub.execute_input":"2025-04-17T00:41:37.345825Z","iopub.status.idle":"2025-04-17T00:42:00.654281Z","shell.execute_reply.started":"2025-04-17T00:41:37.345796Z","shell.execute_reply":"2025-04-17T00:42:00.653454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c9fa11d0ac4e6686aa66f34d259a16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ba6f4a93dc4f7aab6ca8754eb067b7"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 759,556 || all params: 125,408,264 || trainable%: 0.6057\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T00:42:09.926062Z","iopub.execute_input":"2025-04-17T00:42:09.926751Z","iopub.status.idle":"2025-04-17T00:42:09.935559Z","shell.execute_reply.started":"2025-04-17T00:42:09.926717Z","shell.execute_reply":"2025-04-17T00:42:09.934564Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=3000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='cosine',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T01:16:55.834244Z","iopub.execute_input":"2025-04-17T01:16:55.834595Z","iopub.status.idle":"2025-04-17T01:42:09.532143Z","shell.execute_reply.started":"2025-04-17T01:16:55.834566Z","shell.execute_reply":"2025-04-17T01:42:09.530987Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 25:08, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.269000</td>\n      <td>0.306359</td>\n      <td>0.892188</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.281800</td>\n      <td>0.326250</td>\n      <td>0.889062</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.280900</td>\n      <td>0.314796</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.245000</td>\n      <td>0.309985</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.257400</td>\n      <td>0.317893</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.240100</td>\n      <td>0.321106</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.260600</td>\n      <td>0.300190</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.270800</td>\n      <td>0.302510</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.253700</td>\n      <td>0.300990</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.248600</td>\n      <td>0.300539</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.268200</td>\n      <td>0.298618</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.264000</td>\n      <td>0.300582</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.289700</td>\n      <td>0.302901</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.258500</td>\n      <td>0.300632</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.273400</td>\n      <td>0.297817</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.292100</td>\n      <td>0.295399</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.233600</td>\n      <td>0.300142</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.256500</td>\n      <td>0.300591</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.248100</td>\n      <td>0.299414</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.307500</td>\n      <td>0.300245</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.268800</td>\n      <td>0.295877</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.268800</td>\n      <td>0.296203</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.242000</td>\n      <td>0.294823</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.269900</td>\n      <td>0.295216</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.255500</td>\n      <td>0.295142</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.244500</td>\n      <td>0.294137</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.242700</td>\n      <td>0.294639</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.246800</td>\n      <td>0.294143</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.255800</td>\n      <td>0.294274</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.242400</td>\n      <td>0.294262</td>\n      <td>0.904687</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"training_args = TrainingArguments(\n    # Core training configs\n    max_steps=4000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args, checkpoint='/kaggle/working/saved_models/checkpoint-3000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:05:24.006601Z","iopub.execute_input":"2025-04-15T16:05:24.006939Z","iopub.status.idle":"2025-04-15T16:14:06.152384Z","shell.execute_reply.started":"2025-04-15T16:05:24.006916Z","shell.execute_reply":"2025-04-15T16:14:06.151345Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nResuming from checkpoint: /kaggle/working/saved_models/checkpoint-3000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4000/4000 08:28, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3100</td>\n      <td>0.279400</td>\n      <td>0.283299</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.266500</td>\n      <td>0.285942</td>\n      <td>0.915625</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.237500</td>\n      <td>0.289334</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.227600</td>\n      <td>0.291630</td>\n      <td>0.917188</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.269200</td>\n      <td>0.289263</td>\n      <td>0.914062</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.280100</td>\n      <td>0.288613</td>\n      <td>0.915625</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.251200</td>\n      <td>0.287335</td>\n      <td>0.917188</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.238500</td>\n      <td>0.286974</td>\n      <td>0.917188</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.272200</td>\n      <td>0.287824</td>\n      <td>0.918750</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.287000</td>\n      <td>0.287646</td>\n      <td>0.921875</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-700'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T01:45:51.006937Z","iopub.execute_input":"2025-04-17T01:45:51.007286Z","iopub.status.idle":"2025-04-17T01:46:57.761642Z","shell.execute_reply.started":"2025-04-17T01:45:51.007253Z","shell.execute_reply":"2025-04-17T01:46:57.760845Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff38838a555342038ab815bb77d789e6"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.23it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-700.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = '/kaggle/working/saved_models/checkpoint-3000/processed_log_history.csv'\n\ndf = pd.read_csv(csv_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T01:46:57.762768Z","iopub.execute_input":"2025-04-17T01:46:57.763020Z","iopub.status.idle":"2025-04-17T01:46:57.795098Z","shell.execute_reply.started":"2025-04-17T01:46:57.762998Z","shell.execute_reply":"2025-04-17T01:46:57.794390Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      0.2690   0.306359   0.912500  0.892188   4.986305e-05  0.013405   \n1    200      0.2818   0.326250   0.904911  0.889062   4.945369e-05  0.026810   \n2    300      0.2809   0.314796   0.900893  0.904687   4.877641e-05  0.040214   \n3    400      0.2450   0.309985   0.915179  0.906250   4.783864e-05  0.053619   \n4    500      0.2574   0.317893   0.909375  0.903125   4.665064e-05  0.067024   \n5    600      0.2401   0.321106   0.916964  0.907813   4.522542e-05  0.080429   \n6    700      0.2606   0.300190   0.913393  0.912500   4.357862e-05  0.093834   \n7    800      0.2708   0.302510   0.905804  0.904687   4.172827e-05  0.107239   \n8    900      0.2537   0.300990   0.909821  0.903125   3.969463e-05  0.120643   \n9   1000      0.2486   0.300539   0.910714  0.907813   3.750000e-05  0.134048   \n10  1100      0.2682   0.298618   0.910714  0.900000   3.516842e-05  0.147453   \n11  1200      0.2640   0.300582   0.909821  0.900000   3.272542e-05  0.160858   \n12  1300      0.2897   0.302901   0.908929  0.896875   3.019779e-05  0.174263   \n13  1400      0.2585   0.300632   0.911161  0.901563   2.761321e-05  0.187668   \n14  1500      0.2734   0.297817   0.907143  0.907813   2.500000e-05  0.201072   \n15  1600      0.2921   0.295399   0.907143  0.903125   2.238679e-05  0.214477   \n16  1700      0.2336   0.300142   0.917857  0.901563   1.980221e-05  0.227882   \n17  1800      0.2565   0.300591   0.912946  0.903125   1.727458e-05  0.241287   \n18  1900      0.2481   0.299414   0.912500  0.906250   1.483158e-05  0.254692   \n19  2000      0.3075   0.300245   0.909375  0.904687   1.250000e-05  0.268097   \n20  2100      0.2688   0.295877   0.906696  0.906250   1.030537e-05  0.281501   \n21  2200      0.2688   0.296203   0.906696  0.904687   8.271735e-06  0.294906   \n22  2300      0.2420   0.294823   0.911607  0.904687   6.421379e-06  0.308311   \n23  2400      0.2699   0.295216   0.907143  0.904687   4.774575e-06  0.321716   \n24  2500      0.2555   0.295142   0.908929  0.906250   3.349365e-06  0.335121   \n25  2600      0.2445   0.294137   0.914286  0.906250   2.161364e-06  0.348525   \n26  2700      0.2427   0.294639   0.912054  0.907813   1.223587e-06  0.361930   \n27  2800      0.2468   0.294143   0.917411  0.904687   5.463100e-07  0.375335   \n28  2900      0.2558   0.294274   0.912500  0.904687   1.369526e-07  0.388740   \n29  3000      0.2424   0.294262   0.914286  0.904687   0.000000e+00  0.402145   \n\n    Loss Spread  Loss Ratio  Acc Spread  Acc Ratio  \n0     -0.037359    0.878054    0.020312   1.022767  \n1     -0.044450    0.863755    0.015848   1.017826  \n2     -0.033896    0.892323   -0.003795   0.995806  \n3     -0.064985    0.790361    0.008929   1.009852  \n4     -0.060493    0.809706    0.006250   1.006920  \n5     -0.081006    0.747729    0.009152   1.010081  \n6     -0.039590    0.868117    0.000893   1.000978  \n7     -0.031710    0.895177    0.001116   1.001234  \n8     -0.047290    0.842886    0.006696   1.007415  \n9     -0.051939    0.827181    0.002902   1.003196  \n10    -0.030418    0.898137    0.010714   1.011905  \n11    -0.036582    0.878295    0.009821   1.010913  \n12    -0.013201    0.956419    0.012054   1.013440  \n13    -0.042132    0.859856    0.009598   1.010646  \n14    -0.024417    0.918014   -0.000670   0.999262  \n15    -0.003299    0.988833    0.004018   1.004449  \n16    -0.066542    0.778298    0.016295   1.018074  \n17    -0.044091    0.853318    0.009821   1.010875  \n18    -0.051314    0.828619    0.006250   1.006897  \n19     0.007255    1.024162    0.004687   1.005181  \n20    -0.027077    0.908485    0.000446   1.000493  \n21    -0.027403    0.907486    0.002009   1.002221  \n22    -0.052823    0.820832    0.006920   1.007649  \n23    -0.025316    0.914247    0.002455   1.002714  \n24    -0.039642    0.865684    0.002679   1.002956  \n25    -0.049637    0.831246    0.008036   1.008867  \n26    -0.051939    0.823721    0.004241   1.004672  \n27    -0.047343    0.839048    0.012723   1.014064  \n28    -0.038474    0.869258    0.007812   1.008636  \n29    -0.051862    0.823756    0.009598   1.010609  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>Loss Spread</th>\n      <th>Loss Ratio</th>\n      <th>Acc Spread</th>\n      <th>Acc Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.2690</td>\n      <td>0.306359</td>\n      <td>0.912500</td>\n      <td>0.892188</td>\n      <td>4.986305e-05</td>\n      <td>0.013405</td>\n      <td>-0.037359</td>\n      <td>0.878054</td>\n      <td>0.020312</td>\n      <td>1.022767</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.2818</td>\n      <td>0.326250</td>\n      <td>0.904911</td>\n      <td>0.889062</td>\n      <td>4.945369e-05</td>\n      <td>0.026810</td>\n      <td>-0.044450</td>\n      <td>0.863755</td>\n      <td>0.015848</td>\n      <td>1.017826</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.2809</td>\n      <td>0.314796</td>\n      <td>0.900893</td>\n      <td>0.904687</td>\n      <td>4.877641e-05</td>\n      <td>0.040214</td>\n      <td>-0.033896</td>\n      <td>0.892323</td>\n      <td>-0.003795</td>\n      <td>0.995806</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.2450</td>\n      <td>0.309985</td>\n      <td>0.915179</td>\n      <td>0.906250</td>\n      <td>4.783864e-05</td>\n      <td>0.053619</td>\n      <td>-0.064985</td>\n      <td>0.790361</td>\n      <td>0.008929</td>\n      <td>1.009852</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.2574</td>\n      <td>0.317893</td>\n      <td>0.909375</td>\n      <td>0.903125</td>\n      <td>4.665064e-05</td>\n      <td>0.067024</td>\n      <td>-0.060493</td>\n      <td>0.809706</td>\n      <td>0.006250</td>\n      <td>1.006920</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.2401</td>\n      <td>0.321106</td>\n      <td>0.916964</td>\n      <td>0.907813</td>\n      <td>4.522542e-05</td>\n      <td>0.080429</td>\n      <td>-0.081006</td>\n      <td>0.747729</td>\n      <td>0.009152</td>\n      <td>1.010081</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.2606</td>\n      <td>0.300190</td>\n      <td>0.913393</td>\n      <td>0.912500</td>\n      <td>4.357862e-05</td>\n      <td>0.093834</td>\n      <td>-0.039590</td>\n      <td>0.868117</td>\n      <td>0.000893</td>\n      <td>1.000978</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.2708</td>\n      <td>0.302510</td>\n      <td>0.905804</td>\n      <td>0.904687</td>\n      <td>4.172827e-05</td>\n      <td>0.107239</td>\n      <td>-0.031710</td>\n      <td>0.895177</td>\n      <td>0.001116</td>\n      <td>1.001234</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.2537</td>\n      <td>0.300990</td>\n      <td>0.909821</td>\n      <td>0.903125</td>\n      <td>3.969463e-05</td>\n      <td>0.120643</td>\n      <td>-0.047290</td>\n      <td>0.842886</td>\n      <td>0.006696</td>\n      <td>1.007415</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2486</td>\n      <td>0.300539</td>\n      <td>0.910714</td>\n      <td>0.907813</td>\n      <td>3.750000e-05</td>\n      <td>0.134048</td>\n      <td>-0.051939</td>\n      <td>0.827181</td>\n      <td>0.002902</td>\n      <td>1.003196</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.2682</td>\n      <td>0.298618</td>\n      <td>0.910714</td>\n      <td>0.900000</td>\n      <td>3.516842e-05</td>\n      <td>0.147453</td>\n      <td>-0.030418</td>\n      <td>0.898137</td>\n      <td>0.010714</td>\n      <td>1.011905</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.2640</td>\n      <td>0.300582</td>\n      <td>0.909821</td>\n      <td>0.900000</td>\n      <td>3.272542e-05</td>\n      <td>0.160858</td>\n      <td>-0.036582</td>\n      <td>0.878295</td>\n      <td>0.009821</td>\n      <td>1.010913</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.2897</td>\n      <td>0.302901</td>\n      <td>0.908929</td>\n      <td>0.896875</td>\n      <td>3.019779e-05</td>\n      <td>0.174263</td>\n      <td>-0.013201</td>\n      <td>0.956419</td>\n      <td>0.012054</td>\n      <td>1.013440</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2585</td>\n      <td>0.300632</td>\n      <td>0.911161</td>\n      <td>0.901563</td>\n      <td>2.761321e-05</td>\n      <td>0.187668</td>\n      <td>-0.042132</td>\n      <td>0.859856</td>\n      <td>0.009598</td>\n      <td>1.010646</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.2734</td>\n      <td>0.297817</td>\n      <td>0.907143</td>\n      <td>0.907813</td>\n      <td>2.500000e-05</td>\n      <td>0.201072</td>\n      <td>-0.024417</td>\n      <td>0.918014</td>\n      <td>-0.000670</td>\n      <td>0.999262</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.2921</td>\n      <td>0.295399</td>\n      <td>0.907143</td>\n      <td>0.903125</td>\n      <td>2.238679e-05</td>\n      <td>0.214477</td>\n      <td>-0.003299</td>\n      <td>0.988833</td>\n      <td>0.004018</td>\n      <td>1.004449</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2336</td>\n      <td>0.300142</td>\n      <td>0.917857</td>\n      <td>0.901563</td>\n      <td>1.980221e-05</td>\n      <td>0.227882</td>\n      <td>-0.066542</td>\n      <td>0.778298</td>\n      <td>0.016295</td>\n      <td>1.018074</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2565</td>\n      <td>0.300591</td>\n      <td>0.912946</td>\n      <td>0.903125</td>\n      <td>1.727458e-05</td>\n      <td>0.241287</td>\n      <td>-0.044091</td>\n      <td>0.853318</td>\n      <td>0.009821</td>\n      <td>1.010875</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2481</td>\n      <td>0.299414</td>\n      <td>0.912500</td>\n      <td>0.906250</td>\n      <td>1.483158e-05</td>\n      <td>0.254692</td>\n      <td>-0.051314</td>\n      <td>0.828619</td>\n      <td>0.006250</td>\n      <td>1.006897</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3075</td>\n      <td>0.300245</td>\n      <td>0.909375</td>\n      <td>0.904687</td>\n      <td>1.250000e-05</td>\n      <td>0.268097</td>\n      <td>0.007255</td>\n      <td>1.024162</td>\n      <td>0.004687</td>\n      <td>1.005181</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2100</td>\n      <td>0.2688</td>\n      <td>0.295877</td>\n      <td>0.906696</td>\n      <td>0.906250</td>\n      <td>1.030537e-05</td>\n      <td>0.281501</td>\n      <td>-0.027077</td>\n      <td>0.908485</td>\n      <td>0.000446</td>\n      <td>1.000493</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2200</td>\n      <td>0.2688</td>\n      <td>0.296203</td>\n      <td>0.906696</td>\n      <td>0.904687</td>\n      <td>8.271735e-06</td>\n      <td>0.294906</td>\n      <td>-0.027403</td>\n      <td>0.907486</td>\n      <td>0.002009</td>\n      <td>1.002221</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2300</td>\n      <td>0.2420</td>\n      <td>0.294823</td>\n      <td>0.911607</td>\n      <td>0.904687</td>\n      <td>6.421379e-06</td>\n      <td>0.308311</td>\n      <td>-0.052823</td>\n      <td>0.820832</td>\n      <td>0.006920</td>\n      <td>1.007649</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2400</td>\n      <td>0.2699</td>\n      <td>0.295216</td>\n      <td>0.907143</td>\n      <td>0.904687</td>\n      <td>4.774575e-06</td>\n      <td>0.321716</td>\n      <td>-0.025316</td>\n      <td>0.914247</td>\n      <td>0.002455</td>\n      <td>1.002714</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2500</td>\n      <td>0.2555</td>\n      <td>0.295142</td>\n      <td>0.908929</td>\n      <td>0.906250</td>\n      <td>3.349365e-06</td>\n      <td>0.335121</td>\n      <td>-0.039642</td>\n      <td>0.865684</td>\n      <td>0.002679</td>\n      <td>1.002956</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2600</td>\n      <td>0.2445</td>\n      <td>0.294137</td>\n      <td>0.914286</td>\n      <td>0.906250</td>\n      <td>2.161364e-06</td>\n      <td>0.348525</td>\n      <td>-0.049637</td>\n      <td>0.831246</td>\n      <td>0.008036</td>\n      <td>1.008867</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2700</td>\n      <td>0.2427</td>\n      <td>0.294639</td>\n      <td>0.912054</td>\n      <td>0.907813</td>\n      <td>1.223587e-06</td>\n      <td>0.361930</td>\n      <td>-0.051939</td>\n      <td>0.823721</td>\n      <td>0.004241</td>\n      <td>1.004672</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2800</td>\n      <td>0.2468</td>\n      <td>0.294143</td>\n      <td>0.917411</td>\n      <td>0.904687</td>\n      <td>5.463100e-07</td>\n      <td>0.375335</td>\n      <td>-0.047343</td>\n      <td>0.839048</td>\n      <td>0.012723</td>\n      <td>1.014064</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2900</td>\n      <td>0.2558</td>\n      <td>0.294274</td>\n      <td>0.912500</td>\n      <td>0.904687</td>\n      <td>1.369526e-07</td>\n      <td>0.388740</td>\n      <td>-0.038474</td>\n      <td>0.869258</td>\n      <td>0.007812</td>\n      <td>1.008636</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3000</td>\n      <td>0.2424</td>\n      <td>0.294262</td>\n      <td>0.914286</td>\n      <td>0.904687</td>\n      <td>0.000000e+00</td>\n      <td>0.402145</td>\n      <td>-0.051862</td>\n      <td>0.823756</td>\n      <td>0.009598</td>\n      <td>1.010609</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}