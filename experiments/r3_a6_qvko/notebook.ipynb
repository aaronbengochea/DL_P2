{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:36:23.312729Z","iopub.execute_input":"2025-04-16T17:36:23.313027Z","iopub.status.idle":"2025-04-16T17:36:24.288871Z","shell.execute_reply.started":"2025-04-16T17:36:23.312993Z","shell.execute_reply":"2025-04-16T17:36:24.287784Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 3.56 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=3,\n    lora_alpha=6,\n    lora_dropout=0.1,\n    bias='none',\n    target_modules=['query', 'value', 'key', 'attention.output.dense'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:02:18.577640Z","iopub.execute_input":"2025-04-16T18:02:18.577993Z","iopub.status.idle":"2025-04-16T18:02:18.895161Z","shell.execute_reply.started":"2025-04-16T18:02:18.577963Z","shell.execute_reply":"2025-04-16T18:02:18.894422Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:02:22.909167Z","iopub.execute_input":"2025-04-16T18:02:22.909567Z","iopub.status.idle":"2025-04-16T18:02:22.918716Z","shell.execute_reply.started":"2025-04-16T18:02:22.909535Z","shell.execute_reply":"2025-04-16T18:02:22.918053Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=3, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=3, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=2000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:02:27.783508Z","iopub.execute_input":"2025-04-16T18:02:27.783816Z","iopub.status.idle":"2025-04-16T18:19:51.458774Z","shell.execute_reply.started":"2025-04-16T18:02:27.783779Z","shell.execute_reply":"2025-04-16T18:19:51.457947Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 17:18, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.363800</td>\n      <td>1.314823</td>\n      <td>0.751563</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.913800</td>\n      <td>0.377567</td>\n      <td>0.878125</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.379300</td>\n      <td>0.347631</td>\n      <td>0.890625</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.286500</td>\n      <td>0.342458</td>\n      <td>0.896875</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.310400</td>\n      <td>0.330961</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.305100</td>\n      <td>0.323496</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.308700</td>\n      <td>0.327059</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.322600</td>\n      <td>0.311033</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.302500</td>\n      <td>0.310219</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.290200</td>\n      <td>0.307278</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.297900</td>\n      <td>0.305043</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.298900</td>\n      <td>0.306472</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.324700</td>\n      <td>0.309441</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.283200</td>\n      <td>0.305224</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.297400</td>\n      <td>0.308263</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.322500</td>\n      <td>0.304336</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.270700</td>\n      <td>0.310504</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.285900</td>\n      <td>0.305831</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.264200</td>\n      <td>0.304012</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.335900</td>\n      <td>0.303548</td>\n      <td>0.903125</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-800'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:40:16.214119Z","iopub.execute_input":"2025-04-16T18:40:16.214475Z","iopub.status.idle":"2025-04-16T18:41:23.649587Z","shell.execute_reply.started":"2025-04-16T18:40:16.214449Z","shell.execute_reply":"2025-04-16T18:41:23.648763Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be4081b6ac64f14ba62308a48cff0b4"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:59<00:00,  4.18it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-800.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = '/kaggle/working/saved_models/checkpoint-2000/processed_log_history.csv'\n\ndf = pd.read_csv(csv_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:41:35.719791Z","iopub.execute_input":"2025-04-16T18:41:35.720096Z","iopub.status.idle":"2025-04-16T18:41:35.756610Z","shell.execute_reply.started":"2025-04-16T18:41:35.720069Z","shell.execute_reply":"2025-04-16T18:41:35.755902Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3638   1.314823   0.387500  0.751563       0.000048  0.013405   \n1    200      0.9138   0.377567   0.763393  0.878125       0.000045  0.026810   \n2    300      0.3793   0.347631   0.876339  0.890625       0.000043  0.040214   \n3    400      0.2865   0.342458   0.901786  0.896875       0.000040  0.053619   \n4    500      0.3104   0.330961   0.896429  0.901563       0.000037  0.067024   \n5    600      0.3051   0.323496   0.896875  0.903125       0.000035  0.080429   \n6    700      0.3087   0.327059   0.899554  0.901563       0.000033  0.093834   \n7    800      0.3226   0.311033   0.893750  0.906250       0.000030  0.107239   \n8    900      0.3025   0.310219   0.903125  0.901563       0.000028  0.120643   \n9   1000      0.2902   0.307278   0.903571  0.904687       0.000025  0.134048   \n10  1100      0.2979   0.305043   0.903571  0.901563       0.000023  0.147453   \n11  1200      0.2989   0.306472   0.903571  0.904687       0.000020  0.160858   \n12  1300      0.3247   0.309441   0.895536  0.904687       0.000017  0.174263   \n13  1400      0.2832   0.305224   0.904018  0.904687       0.000015  0.187668   \n14  1500      0.2974   0.308263   0.900000  0.904687       0.000013  0.201072   \n15  1600      0.3225   0.304336   0.898661  0.904687       0.000010  0.214477   \n16  1700      0.2707   0.310504   0.909821  0.904687       0.000008  0.227882   \n17  1800      0.2859   0.305831   0.902679  0.903125       0.000005  0.241287   \n18  1900      0.2642   0.304012   0.906250  0.904687       0.000003  0.254692   \n19  2000      0.3359   0.303548   0.900000  0.903125       0.000000  0.268097   \n\n    Loss Spread  Loss Ratio  Acc Spread  Acc Ratio  \n0      0.048977    1.037250   -0.364063   0.515593  \n1      0.536233    2.420233   -0.114732   0.869344  \n2      0.031669    1.091100   -0.014286   0.983960  \n3     -0.055958    0.836599    0.004911   1.005475  \n4     -0.020561    0.937874   -0.005134   0.994306  \n5     -0.018396    0.943134   -0.006250   0.993080  \n6     -0.018359    0.943868   -0.002009   0.997772  \n7      0.011567    1.037188   -0.012500   0.986207  \n8     -0.007719    0.975119    0.001562   1.001733  \n9     -0.017078    0.944421   -0.001116   0.998766  \n10    -0.007143    0.976584    0.002009   1.002228  \n11    -0.007572    0.975293   -0.001116   0.998766  \n12     0.015259    1.049312   -0.009152   0.989884  \n13    -0.022024    0.927844   -0.000670   0.999260  \n14    -0.010863    0.964762   -0.004687   0.994819  \n15     0.018164    1.059685   -0.006027   0.993338  \n16    -0.039804    0.871807    0.005134   1.005675  \n17    -0.019931    0.934830   -0.000446   0.999506  \n18    -0.039812    0.869044    0.001563   1.001727  \n19     0.032352    1.106581   -0.003125   0.996540  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>Loss Spread</th>\n      <th>Loss Ratio</th>\n      <th>Acc Spread</th>\n      <th>Acc Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3638</td>\n      <td>1.314823</td>\n      <td>0.387500</td>\n      <td>0.751563</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.048977</td>\n      <td>1.037250</td>\n      <td>-0.364063</td>\n      <td>0.515593</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>0.9138</td>\n      <td>0.377567</td>\n      <td>0.763393</td>\n      <td>0.878125</td>\n      <td>0.000045</td>\n      <td>0.026810</td>\n      <td>0.536233</td>\n      <td>2.420233</td>\n      <td>-0.114732</td>\n      <td>0.869344</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.3793</td>\n      <td>0.347631</td>\n      <td>0.876339</td>\n      <td>0.890625</td>\n      <td>0.000043</td>\n      <td>0.040214</td>\n      <td>0.031669</td>\n      <td>1.091100</td>\n      <td>-0.014286</td>\n      <td>0.983960</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.2865</td>\n      <td>0.342458</td>\n      <td>0.901786</td>\n      <td>0.896875</td>\n      <td>0.000040</td>\n      <td>0.053619</td>\n      <td>-0.055958</td>\n      <td>0.836599</td>\n      <td>0.004911</td>\n      <td>1.005475</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.3104</td>\n      <td>0.330961</td>\n      <td>0.896429</td>\n      <td>0.901563</td>\n      <td>0.000037</td>\n      <td>0.067024</td>\n      <td>-0.020561</td>\n      <td>0.937874</td>\n      <td>-0.005134</td>\n      <td>0.994306</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.3051</td>\n      <td>0.323496</td>\n      <td>0.896875</td>\n      <td>0.903125</td>\n      <td>0.000035</td>\n      <td>0.080429</td>\n      <td>-0.018396</td>\n      <td>0.943134</td>\n      <td>-0.006250</td>\n      <td>0.993080</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3087</td>\n      <td>0.327059</td>\n      <td>0.899554</td>\n      <td>0.901563</td>\n      <td>0.000033</td>\n      <td>0.093834</td>\n      <td>-0.018359</td>\n      <td>0.943868</td>\n      <td>-0.002009</td>\n      <td>0.997772</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3226</td>\n      <td>0.311033</td>\n      <td>0.893750</td>\n      <td>0.906250</td>\n      <td>0.000030</td>\n      <td>0.107239</td>\n      <td>0.011567</td>\n      <td>1.037188</td>\n      <td>-0.012500</td>\n      <td>0.986207</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.3025</td>\n      <td>0.310219</td>\n      <td>0.903125</td>\n      <td>0.901563</td>\n      <td>0.000028</td>\n      <td>0.120643</td>\n      <td>-0.007719</td>\n      <td>0.975119</td>\n      <td>0.001562</td>\n      <td>1.001733</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2902</td>\n      <td>0.307278</td>\n      <td>0.903571</td>\n      <td>0.904687</td>\n      <td>0.000025</td>\n      <td>0.134048</td>\n      <td>-0.017078</td>\n      <td>0.944421</td>\n      <td>-0.001116</td>\n      <td>0.998766</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.2979</td>\n      <td>0.305043</td>\n      <td>0.903571</td>\n      <td>0.901563</td>\n      <td>0.000023</td>\n      <td>0.147453</td>\n      <td>-0.007143</td>\n      <td>0.976584</td>\n      <td>0.002009</td>\n      <td>1.002228</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.2989</td>\n      <td>0.306472</td>\n      <td>0.903571</td>\n      <td>0.904687</td>\n      <td>0.000020</td>\n      <td>0.160858</td>\n      <td>-0.007572</td>\n      <td>0.975293</td>\n      <td>-0.001116</td>\n      <td>0.998766</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3247</td>\n      <td>0.309441</td>\n      <td>0.895536</td>\n      <td>0.904687</td>\n      <td>0.000017</td>\n      <td>0.174263</td>\n      <td>0.015259</td>\n      <td>1.049312</td>\n      <td>-0.009152</td>\n      <td>0.989884</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2832</td>\n      <td>0.305224</td>\n      <td>0.904018</td>\n      <td>0.904687</td>\n      <td>0.000015</td>\n      <td>0.187668</td>\n      <td>-0.022024</td>\n      <td>0.927844</td>\n      <td>-0.000670</td>\n      <td>0.999260</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.2974</td>\n      <td>0.308263</td>\n      <td>0.900000</td>\n      <td>0.904687</td>\n      <td>0.000013</td>\n      <td>0.201072</td>\n      <td>-0.010863</td>\n      <td>0.964762</td>\n      <td>-0.004687</td>\n      <td>0.994819</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3225</td>\n      <td>0.304336</td>\n      <td>0.898661</td>\n      <td>0.904687</td>\n      <td>0.000010</td>\n      <td>0.214477</td>\n      <td>0.018164</td>\n      <td>1.059685</td>\n      <td>-0.006027</td>\n      <td>0.993338</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2707</td>\n      <td>0.310504</td>\n      <td>0.909821</td>\n      <td>0.904687</td>\n      <td>0.000008</td>\n      <td>0.227882</td>\n      <td>-0.039804</td>\n      <td>0.871807</td>\n      <td>0.005134</td>\n      <td>1.005675</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2859</td>\n      <td>0.305831</td>\n      <td>0.902679</td>\n      <td>0.903125</td>\n      <td>0.000005</td>\n      <td>0.241287</td>\n      <td>-0.019931</td>\n      <td>0.934830</td>\n      <td>-0.000446</td>\n      <td>0.999506</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2642</td>\n      <td>0.304012</td>\n      <td>0.906250</td>\n      <td>0.904687</td>\n      <td>0.000003</td>\n      <td>0.254692</td>\n      <td>-0.039812</td>\n      <td>0.869044</td>\n      <td>0.001563</td>\n      <td>1.001727</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3359</td>\n      <td>0.303548</td>\n      <td>0.900000</td>\n      <td>0.903125</td>\n      <td>0.000000</td>\n      <td>0.268097</td>\n      <td>0.032352</td>\n      <td>1.106581</td>\n      <td>-0.003125</td>\n      <td>0.996540</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}