{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/timothycao/agnews-classifier.git\n%cd agnews-classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:49:51.451727Z","iopub.execute_input":"2025-04-15T01:49:51.452016Z","iopub.status.idle":"2025-04-15T01:49:52.050559Z","shell.execute_reply.started":"2025-04-15T01:49:51.451985Z","shell.execute_reply":"2025-04-15T01:49:52.049617Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'agnews-classifier'...\nremote: Enumerating objects: 37, done.\u001b[K\nremote: Counting objects: 100% (37/37), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 37 (delta 17), reused 28 (delta 9), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (37/37), 10.68 KiB | 3.56 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/agnews-classifier\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model\n\nfrom peft import LoraConfig\nfrom model import create_lora_model\n\nlora_config = LoraConfig(\n    r=1,\n    lora_alpha=3,\n    lora_dropout=0.1,\n    bias='lora_only',\n    target_modules=['query', 'key', 'value', 'attention.output.dense'],\n    task_type='SEQ_CLS',\n    use_rslora = False # regular LoRA: lora_alpha/r, rs-LoRA: lora_alpha/sqrt(r) -> normalization technique\n)\n\nmodel = create_lora_model(lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:49:55.736356Z","iopub.execute_input":"2025-04-15T01:49:55.736703Z","iopub.status.idle":"2025-04-15T01:50:24.098083Z","shell.execute_reply.started":"2025-04-15T01:49:55.736676Z","shell.execute_reply":"2025-04-15T01:50:24.097216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4e3c22af2b4a4ba1d88898b3e94861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d14860a04f4f8abe048e5ad7be9ed9"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 704,260 || all params: 125,316,104 || trainable%: 0.5620\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:50:24.099108Z","iopub.execute_input":"2025-04-15T01:50:24.099672Z","iopub.status.idle":"2025-04-15T01:50:24.110463Z","shell.execute_reply.started":"2025-04-15T01:50:24.099645Z","shell.execute_reply":"2025-04-15T01:50:24.109463Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Training\n\nfrom transformers import TrainingArguments\nfrom train import main as train\n\ntraining_args = TrainingArguments(\n    # Core training configs\n    max_steps=3000,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    optim='adamw_torch',\n    learning_rate=5e-5,\n    lr_scheduler_type='linear',\n    \n    # Logging, evaluation, and checkpointing\n    output_dir='/kaggle/working/saved_models',\n    logging_strategy='steps',\n    logging_steps=100,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n\n    # Miscellaneous\n    report_to='none',\n    dataloader_num_workers=4,\n    gradient_checkpointing=False,\n    gradient_checkpointing_kwargs={'use_reentrant':True}\n)\n\ntrain(model, training_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T01:50:34.885911Z","iopub.execute_input":"2025-04-15T01:50:34.886235Z","iopub.status.idle":"2025-04-15T02:17:37.503240Z","shell.execute_reply.started":"2025-04-15T01:50:34.886207Z","shell.execute_reply":"2025-04-15T02:17:37.502255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a136862110430bb1071a801c9c6de4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3df5dcfba1184a66b07e5663d0a3854b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08cf8b10fe24494d9c0d0b2b1054e4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75532016ec56493bb74b3837678c0665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d644fe7bcd472099373e89b9edff9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"676aa4f041774ec6bab2d90600d6b02a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3e2e7a98954fc6b3ac037c0ef99d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c44dc7bad6d46cdb5902c1190611874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4741687ce640759c89bbff96abf323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972532ff10eb41fa97328bd58329af0b"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 25:46, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.363400</td>\n      <td>1.319008</td>\n      <td>0.734375</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.038000</td>\n      <td>0.459037</td>\n      <td>0.862500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.402700</td>\n      <td>0.358042</td>\n      <td>0.884375</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.299700</td>\n      <td>0.349030</td>\n      <td>0.889062</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.319900</td>\n      <td>0.335329</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.308800</td>\n      <td>0.332608</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.308300</td>\n      <td>0.335975</td>\n      <td>0.898438</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.338200</td>\n      <td>0.319524</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.306700</td>\n      <td>0.315760</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.297400</td>\n      <td>0.312396</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.300400</td>\n      <td>0.307846</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.302200</td>\n      <td>0.309713</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.335500</td>\n      <td>0.314700</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.292200</td>\n      <td>0.307941</td>\n      <td>0.903125</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.299600</td>\n      <td>0.317097</td>\n      <td>0.904687</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.326700</td>\n      <td>0.309703</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.274600</td>\n      <td>0.310100</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.289300</td>\n      <td>0.306207</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.261900</td>\n      <td>0.305059</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.332200</td>\n      <td>0.304925</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.302300</td>\n      <td>0.303415</td>\n      <td>0.910937</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.298000</td>\n      <td>0.301830</td>\n      <td>0.909375</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.273300</td>\n      <td>0.305416</td>\n      <td>0.901563</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.303600</td>\n      <td>0.306954</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.277300</td>\n      <td>0.303026</td>\n      <td>0.907813</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.265900</td>\n      <td>0.301943</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.261000</td>\n      <td>0.302369</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.271200</td>\n      <td>0.299387</td>\n      <td>0.914062</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.271300</td>\n      <td>0.300902</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.282300</td>\n      <td>0.300565</td>\n      <td>0.912500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Inference\n\nfrom inference import main as inference\n\ndata_path = '/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl'\ncheckpoint = '/kaggle/working/saved_models/checkpoint-2800'\noutput_dir = '/kaggle/working/saved_predictions'\n\ninference(data_path, checkpoint, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T02:30:25.771676Z","iopub.execute_input":"2025-04-15T02:30:25.772116Z","iopub.status.idle":"2025-04-15T02:31:34.119482Z","shell.execute_reply.started":"2025-04-15T02:30:25.772081Z","shell.execute_reply":"2025-04-15T02:31:34.118531Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c2e6903a6c4c988b0fcfe4fd8346ef"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [01:00<00:00,  4.14it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/saved_predictions/predictions_checkpoint-2800.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = '/kaggle/working/saved_models/checkpoint-3000/processed_log_history.csv'\n\ndf = pd.read_csv(csv_path)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T02:17:37.506108Z","iopub.execute_input":"2025-04-15T02:17:37.506388Z","iopub.status.idle":"2025-04-15T02:17:37.546109Z","shell.execute_reply.started":"2025-04-15T02:17:37.506361Z","shell.execute_reply":"2025-04-15T02:17:37.545231Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"    Step  Train Loss  Test Loss  Train Acc  Test Acc  Learning Rate    Epochs  \\\n0    100      1.3634   1.319008   0.380625  0.734375       0.000048  0.013405   \n1    200      1.0380   0.459037   0.744643  0.862500       0.000047  0.026810   \n2    300      0.4027   0.358042   0.868750  0.884375       0.000045  0.040214   \n3    400      0.2997   0.349030   0.898661  0.889062       0.000043  0.053619   \n4    500      0.3199   0.335329   0.894643  0.898438       0.000042  0.067024   \n5    600      0.3088   0.332608   0.894196  0.900000       0.000040  0.080429   \n6    700      0.3083   0.335975   0.898661  0.898438       0.000038  0.093834   \n7    800      0.3382   0.319524   0.887946  0.901563       0.000037  0.107239   \n8    900      0.3067   0.315760   0.897768  0.904687       0.000035  0.120643   \n9   1000      0.2974   0.312396   0.900893  0.906250       0.000033  0.134048   \n10  1100      0.3004   0.307846   0.901339  0.909375       0.000032  0.147453   \n11  1200      0.3022   0.309713   0.906696  0.903125       0.000030  0.160858   \n12  1300      0.3355   0.314700   0.897321  0.906250       0.000028  0.174263   \n13  1400      0.2922   0.307941   0.899554  0.903125       0.000027  0.187668   \n14  1500      0.2996   0.317097   0.898661  0.904687       0.000025  0.201072   \n15  1600      0.3267   0.309703   0.898661  0.910937       0.000023  0.214477   \n16  1700      0.2746   0.310100   0.911607  0.907813       0.000022  0.227882   \n17  1800      0.2893   0.306207   0.907143  0.910937       0.000020  0.241287   \n18  1900      0.2619   0.305059   0.907589  0.909375       0.000018  0.254692   \n19  2000      0.3322   0.304925   0.901339  0.906250       0.000017  0.268097   \n20  2100      0.3023   0.303415   0.897321  0.910937       0.000015  0.281501   \n21  2200      0.2980   0.301830   0.895982  0.909375       0.000013  0.294906   \n22  2300      0.2733   0.305416   0.905804  0.901563       0.000012  0.308311   \n23  2400      0.3036   0.306954   0.897768  0.900000       0.000010  0.321716   \n24  2500      0.2773   0.303026   0.899554  0.907813       0.000008  0.335121   \n25  2600      0.2659   0.301943   0.908929  0.912500       0.000007  0.348525   \n26  2700      0.2610   0.302369   0.907143  0.912500       0.000005  0.361930   \n27  2800      0.2712   0.299387   0.910714  0.914062       0.000003  0.375335   \n28  2900      0.2713   0.300902   0.908482  0.912500       0.000002  0.388740   \n29  3000      0.2823   0.300565   0.908482  0.912500       0.000000  0.402145   \n\n    Loss Spread  Loss Ratio  Acc Spread  Acc Ratio  \n0      0.044392    1.033655   -0.353750   0.518298  \n1      0.578963    2.261255   -0.117857   0.863354  \n2      0.044658    1.124727   -0.015625   0.982332  \n3     -0.049330    0.858665    0.009598   1.010796  \n4     -0.015429    0.953989   -0.003795   0.995776  \n5     -0.023808    0.928421   -0.005804   0.993552  \n6     -0.027675    0.917626    0.000223   1.000248  \n7      0.018676    1.058449   -0.013616   0.984897  \n8     -0.009060    0.971308   -0.006920   0.992351  \n9     -0.014996    0.951997   -0.005357   0.994089  \n10    -0.007446    0.975813   -0.008036   0.991163  \n11    -0.007513    0.975742    0.003571   1.003955  \n12     0.020800    1.066093   -0.008929   0.990148  \n13    -0.015741    0.948884   -0.003571   0.996045  \n14    -0.017497    0.944820   -0.006027   0.993338  \n15     0.016997    1.054880   -0.012277   0.986523  \n16    -0.035500    0.885520    0.003795   1.004180  \n17    -0.016907    0.944786   -0.003795   0.995834  \n18    -0.043159    0.858521   -0.001786   0.998036  \n19     0.027275    1.089448   -0.004911   0.994581  \n20    -0.001115    0.996325   -0.013616   0.985053  \n21    -0.003830    0.987310   -0.013393   0.985272  \n22    -0.032116    0.894844    0.004241   1.004704  \n23    -0.003354    0.989072   -0.002232   0.997520  \n24    -0.025726    0.915103   -0.008259   0.990902  \n25    -0.036043    0.880629   -0.003571   0.996086  \n26    -0.041369    0.863183   -0.005357   0.994129  \n27    -0.028187    0.905851   -0.003348   0.996337  \n28    -0.029602    0.901623   -0.004018   0.995597  \n29    -0.018265    0.939230   -0.004018   0.995597  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Step</th>\n      <th>Train Loss</th>\n      <th>Test Loss</th>\n      <th>Train Acc</th>\n      <th>Test Acc</th>\n      <th>Learning Rate</th>\n      <th>Epochs</th>\n      <th>Loss Spread</th>\n      <th>Loss Ratio</th>\n      <th>Acc Spread</th>\n      <th>Acc Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.3634</td>\n      <td>1.319008</td>\n      <td>0.380625</td>\n      <td>0.734375</td>\n      <td>0.000048</td>\n      <td>0.013405</td>\n      <td>0.044392</td>\n      <td>1.033655</td>\n      <td>-0.353750</td>\n      <td>0.518298</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>1.0380</td>\n      <td>0.459037</td>\n      <td>0.744643</td>\n      <td>0.862500</td>\n      <td>0.000047</td>\n      <td>0.026810</td>\n      <td>0.578963</td>\n      <td>2.261255</td>\n      <td>-0.117857</td>\n      <td>0.863354</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.4027</td>\n      <td>0.358042</td>\n      <td>0.868750</td>\n      <td>0.884375</td>\n      <td>0.000045</td>\n      <td>0.040214</td>\n      <td>0.044658</td>\n      <td>1.124727</td>\n      <td>-0.015625</td>\n      <td>0.982332</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>400</td>\n      <td>0.2997</td>\n      <td>0.349030</td>\n      <td>0.898661</td>\n      <td>0.889062</td>\n      <td>0.000043</td>\n      <td>0.053619</td>\n      <td>-0.049330</td>\n      <td>0.858665</td>\n      <td>0.009598</td>\n      <td>1.010796</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>0.3199</td>\n      <td>0.335329</td>\n      <td>0.894643</td>\n      <td>0.898438</td>\n      <td>0.000042</td>\n      <td>0.067024</td>\n      <td>-0.015429</td>\n      <td>0.953989</td>\n      <td>-0.003795</td>\n      <td>0.995776</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>600</td>\n      <td>0.3088</td>\n      <td>0.332608</td>\n      <td>0.894196</td>\n      <td>0.900000</td>\n      <td>0.000040</td>\n      <td>0.080429</td>\n      <td>-0.023808</td>\n      <td>0.928421</td>\n      <td>-0.005804</td>\n      <td>0.993552</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>700</td>\n      <td>0.3083</td>\n      <td>0.335975</td>\n      <td>0.898661</td>\n      <td>0.898438</td>\n      <td>0.000038</td>\n      <td>0.093834</td>\n      <td>-0.027675</td>\n      <td>0.917626</td>\n      <td>0.000223</td>\n      <td>1.000248</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>800</td>\n      <td>0.3382</td>\n      <td>0.319524</td>\n      <td>0.887946</td>\n      <td>0.901563</td>\n      <td>0.000037</td>\n      <td>0.107239</td>\n      <td>0.018676</td>\n      <td>1.058449</td>\n      <td>-0.013616</td>\n      <td>0.984897</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>0.3067</td>\n      <td>0.315760</td>\n      <td>0.897768</td>\n      <td>0.904687</td>\n      <td>0.000035</td>\n      <td>0.120643</td>\n      <td>-0.009060</td>\n      <td>0.971308</td>\n      <td>-0.006920</td>\n      <td>0.992351</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1000</td>\n      <td>0.2974</td>\n      <td>0.312396</td>\n      <td>0.900893</td>\n      <td>0.906250</td>\n      <td>0.000033</td>\n      <td>0.134048</td>\n      <td>-0.014996</td>\n      <td>0.951997</td>\n      <td>-0.005357</td>\n      <td>0.994089</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1100</td>\n      <td>0.3004</td>\n      <td>0.307846</td>\n      <td>0.901339</td>\n      <td>0.909375</td>\n      <td>0.000032</td>\n      <td>0.147453</td>\n      <td>-0.007446</td>\n      <td>0.975813</td>\n      <td>-0.008036</td>\n      <td>0.991163</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1200</td>\n      <td>0.3022</td>\n      <td>0.309713</td>\n      <td>0.906696</td>\n      <td>0.903125</td>\n      <td>0.000030</td>\n      <td>0.160858</td>\n      <td>-0.007513</td>\n      <td>0.975742</td>\n      <td>0.003571</td>\n      <td>1.003955</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1300</td>\n      <td>0.3355</td>\n      <td>0.314700</td>\n      <td>0.897321</td>\n      <td>0.906250</td>\n      <td>0.000028</td>\n      <td>0.174263</td>\n      <td>0.020800</td>\n      <td>1.066093</td>\n      <td>-0.008929</td>\n      <td>0.990148</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1400</td>\n      <td>0.2922</td>\n      <td>0.307941</td>\n      <td>0.899554</td>\n      <td>0.903125</td>\n      <td>0.000027</td>\n      <td>0.187668</td>\n      <td>-0.015741</td>\n      <td>0.948884</td>\n      <td>-0.003571</td>\n      <td>0.996045</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1500</td>\n      <td>0.2996</td>\n      <td>0.317097</td>\n      <td>0.898661</td>\n      <td>0.904687</td>\n      <td>0.000025</td>\n      <td>0.201072</td>\n      <td>-0.017497</td>\n      <td>0.944820</td>\n      <td>-0.006027</td>\n      <td>0.993338</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1600</td>\n      <td>0.3267</td>\n      <td>0.309703</td>\n      <td>0.898661</td>\n      <td>0.910937</td>\n      <td>0.000023</td>\n      <td>0.214477</td>\n      <td>0.016997</td>\n      <td>1.054880</td>\n      <td>-0.012277</td>\n      <td>0.986523</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1700</td>\n      <td>0.2746</td>\n      <td>0.310100</td>\n      <td>0.911607</td>\n      <td>0.907813</td>\n      <td>0.000022</td>\n      <td>0.227882</td>\n      <td>-0.035500</td>\n      <td>0.885520</td>\n      <td>0.003795</td>\n      <td>1.004180</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1800</td>\n      <td>0.2893</td>\n      <td>0.306207</td>\n      <td>0.907143</td>\n      <td>0.910937</td>\n      <td>0.000020</td>\n      <td>0.241287</td>\n      <td>-0.016907</td>\n      <td>0.944786</td>\n      <td>-0.003795</td>\n      <td>0.995834</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1900</td>\n      <td>0.2619</td>\n      <td>0.305059</td>\n      <td>0.907589</td>\n      <td>0.909375</td>\n      <td>0.000018</td>\n      <td>0.254692</td>\n      <td>-0.043159</td>\n      <td>0.858521</td>\n      <td>-0.001786</td>\n      <td>0.998036</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2000</td>\n      <td>0.3322</td>\n      <td>0.304925</td>\n      <td>0.901339</td>\n      <td>0.906250</td>\n      <td>0.000017</td>\n      <td>0.268097</td>\n      <td>0.027275</td>\n      <td>1.089448</td>\n      <td>-0.004911</td>\n      <td>0.994581</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2100</td>\n      <td>0.3023</td>\n      <td>0.303415</td>\n      <td>0.897321</td>\n      <td>0.910937</td>\n      <td>0.000015</td>\n      <td>0.281501</td>\n      <td>-0.001115</td>\n      <td>0.996325</td>\n      <td>-0.013616</td>\n      <td>0.985053</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2200</td>\n      <td>0.2980</td>\n      <td>0.301830</td>\n      <td>0.895982</td>\n      <td>0.909375</td>\n      <td>0.000013</td>\n      <td>0.294906</td>\n      <td>-0.003830</td>\n      <td>0.987310</td>\n      <td>-0.013393</td>\n      <td>0.985272</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2300</td>\n      <td>0.2733</td>\n      <td>0.305416</td>\n      <td>0.905804</td>\n      <td>0.901563</td>\n      <td>0.000012</td>\n      <td>0.308311</td>\n      <td>-0.032116</td>\n      <td>0.894844</td>\n      <td>0.004241</td>\n      <td>1.004704</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2400</td>\n      <td>0.3036</td>\n      <td>0.306954</td>\n      <td>0.897768</td>\n      <td>0.900000</td>\n      <td>0.000010</td>\n      <td>0.321716</td>\n      <td>-0.003354</td>\n      <td>0.989072</td>\n      <td>-0.002232</td>\n      <td>0.997520</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2500</td>\n      <td>0.2773</td>\n      <td>0.303026</td>\n      <td>0.899554</td>\n      <td>0.907813</td>\n      <td>0.000008</td>\n      <td>0.335121</td>\n      <td>-0.025726</td>\n      <td>0.915103</td>\n      <td>-0.008259</td>\n      <td>0.990902</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2600</td>\n      <td>0.2659</td>\n      <td>0.301943</td>\n      <td>0.908929</td>\n      <td>0.912500</td>\n      <td>0.000007</td>\n      <td>0.348525</td>\n      <td>-0.036043</td>\n      <td>0.880629</td>\n      <td>-0.003571</td>\n      <td>0.996086</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2700</td>\n      <td>0.2610</td>\n      <td>0.302369</td>\n      <td>0.907143</td>\n      <td>0.912500</td>\n      <td>0.000005</td>\n      <td>0.361930</td>\n      <td>-0.041369</td>\n      <td>0.863183</td>\n      <td>-0.005357</td>\n      <td>0.994129</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2800</td>\n      <td>0.2712</td>\n      <td>0.299387</td>\n      <td>0.910714</td>\n      <td>0.914062</td>\n      <td>0.000003</td>\n      <td>0.375335</td>\n      <td>-0.028187</td>\n      <td>0.905851</td>\n      <td>-0.003348</td>\n      <td>0.996337</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2900</td>\n      <td>0.2713</td>\n      <td>0.300902</td>\n      <td>0.908482</td>\n      <td>0.912500</td>\n      <td>0.000002</td>\n      <td>0.388740</td>\n      <td>-0.029602</td>\n      <td>0.901623</td>\n      <td>-0.004018</td>\n      <td>0.995597</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>3000</td>\n      <td>0.2823</td>\n      <td>0.300565</td>\n      <td>0.908482</td>\n      <td>0.912500</td>\n      <td>0.000000</td>\n      <td>0.402145</td>\n      <td>-0.018265</td>\n      <td>0.939230</td>\n      <td>-0.004018</td>\n      <td>0.995597</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}